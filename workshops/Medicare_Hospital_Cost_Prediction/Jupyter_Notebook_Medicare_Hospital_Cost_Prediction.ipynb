{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Built-In Linear Learner Algorithm - Model for Predicting Medicare Average Hospital Spending:\n",
    "In this notebook, we show how to build a Model to predict Medicare spending per beneficiary at a hospital based on state and national average costs using Amazon SageMaker. This notebook contains code instructions for all the steps for building, training and deploying a machine learning model using SageMaker built-in Linear Learner algorithm. In this example, the notebook is created on a \"ml.c4.xlarge\" instance.\n",
    "\n",
    "## Learning Objectives:\n",
    "This workshop covers both the preprocessing using SageMaker algorithms and other Python libraries. The major learnings are:\n",
    "\n",
    "1. Load data into SageMaker Notebooks\n",
    "2. Perform basic preprocessing including: feature cleaning, normalization and basic feature engineering.\n",
    "3. Perform basic feature selection/subsampling.\n",
    "4. Perform exploratory data analysis.\n",
    "5. Build, train, and deploy Linear Learner regression model.\n",
    "\n",
    "## Business Problem:\n",
    "Medicare is a national health insurance program, administered by the Center for Medicare and Medicaid Services (CMS). This is a primary health insurance for Americans who are aged 65 and older. Medicare has published historical data showing hospital’s average spending for Medicare Part A and Part B claims based on different claim types and claim periods covering 1 to 3 days prior to hospital admission up to 30 days after discharge from hospital admission. These hospital spending are price standardized and non-risk adjusted, since risk adjustment is done at the episode level of the claims spanning the entire period during the episode. The hospital average costs are listed against the corresponding state level average cost and national level average cost.\n",
    "\n",
    "In this notebook, the data is used to build a machine learning model using Amazon SageMaker built-in Linear Learner algorithm, which predicts average hospital spending cost based on the average state level spending and average national level spending. The predicted cost can be used for purposes of budget and for negotiating pricing with the hospitals. From the hospital’s perspective, the predicted average hospital spending provides visibility to claim financials that can be used by the hospitals to increase their efficiency and level of care.\n",
    "\n",
    "## Public Dataset Used:\n",
    "Medicare has published dataset showing average hospital spending on Medicare Part A and Part B claims. Both the links below refer to the same data set, one is listed in the healthdata.gov site and the other is listed at the data.medicare.gov site. The data dictionary is described in the link marked as #2 below. The dataset has hospital spending data from the year 2018 and has 67,826 data rows spanning across 13 columns. For the purposes of our analysis and machine learning, we use the dataset in csv (Comma Separated Values) format.\n",
    "1.\thttps://healthdata.gov/dataset/medicare-hospital-spending-claim\n",
    "2.\thttps://data.medicare.gov/Hospital-Compare/Medicare-Hospital-Spending-by-Claim/nrth-mfg3\n",
    "\n",
    "A direct link to download the dataset to local computer can be accessed at this link - https://data.medicare.gov/api/views/nrth-mfg3/rows.csv?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install correct version of libraries\n",
    "%pip install 'sagemaker==2.48.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import seaborn as sn\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3Downloader.download(\n",
    "    s3_uri=\"s3://aws-hcls-ml/workshop/immersion_day_workshop_data_DO_NOT_DELETE/data/medicare_data_07_13_2021/Medicare_Hospital_Spending_by_Claim.csv\",\n",
    "    local_path=\"data\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing on the Raw Dataset:\n",
    "In this section we read the raw csv data set into a pandas data frame. We inspect the data using pandas head() function. We do data pre-processing using feature encoding, feature engineering, column renaming, dropping some columns that have no relevance to the prediction of `Avg_Hosp` cost and examining there are no missing values in the data set.\n",
    "\n",
    "**Note**: Many of these transformations and preprocessing steps are for demonstration purposes only and may not correspond to the optimal transformations for a specific column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into panda dataframe and save it to another table so we can keep a copy of the original dataset\n",
    "# In our example we use the dataframe called table1 for all pre-processing, while the dataframe table\n",
    "# maintains a copy of the original data\n",
    "\n",
    "table = pd.read_csv(\"data/Medicare_Hospital_Spending_by_Claim.csv\")\n",
    "table1 = table.copy()\n",
    "table1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode column \"State\"\n",
    "\n",
    "replace_map = {\n",
    "    \"State\": {\n",
    "        \"AK\": 1,\n",
    "        \"AL\": 2,\n",
    "        \"AR\": 3,\n",
    "        \"AZ\": 4,\n",
    "        \"CA\": 5,\n",
    "        \"CO\": 6,\n",
    "        \"CT\": 7,\n",
    "        \"DC\": 8,\n",
    "        \"DE\": 9,\n",
    "        \"FL\": 10,\n",
    "        \"GA\": 11,\n",
    "        \"HI\": 12,\n",
    "        \"IA\": 13,\n",
    "        \"ID\": 14,\n",
    "        \"IL\": 15,\n",
    "        \"IN\": 16,\n",
    "        \"KS\": 17,\n",
    "        \"KY\": 18,\n",
    "        \"LA\": 19,\n",
    "        \"MA\": 20,\n",
    "        \"ME\": 21,\n",
    "        \"MI\": 22,\n",
    "        \"MN\": 23,\n",
    "        \"MO\": 24,\n",
    "        \"MS\": 25,\n",
    "        \"MT\": 26,\n",
    "        \"NC\": 27,\n",
    "        \"ND\": 28,\n",
    "        \"NE\": 29,\n",
    "        \"NH\": 30,\n",
    "        \"NJ\": 31,\n",
    "        \"NM\": 32,\n",
    "        \"NV\": 33,\n",
    "        \"NY\": 34,\n",
    "        \"OH\": 35,\n",
    "        \"OK\": 36,\n",
    "        \"OR\": 37,\n",
    "        \"PA\": 38,\n",
    "        \"RI\": 39,\n",
    "        \"SC\": 40,\n",
    "        \"SD\": 41,\n",
    "        \"TN\": 42,\n",
    "        \"TX\": 43,\n",
    "        \"UT\": 44,\n",
    "        \"VA\": 45,\n",
    "        \"VT\": 46,\n",
    "        \"WA\": 47,\n",
    "        \"WI\": 48,\n",
    "        \"WV\": 49,\n",
    "        \"WY\": 50,\n",
    "    }\n",
    "}\n",
    "table1.replace(replace_map, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode column \"Period\"\n",
    "\n",
    "replace_map = {\n",
    "    \"Period\": {\n",
    "        \"1 to 3 days Prior to Index Hospital Admission\": 1,\n",
    "        \"During Index Hospital Admission\": 2,\n",
    "        \"1 through 30 days After Discharge from Index Hospital Admission\": 3,\n",
    "        \"Complete Episode\": 4,\n",
    "    }\n",
    "}\n",
    "table1.replace(replace_map, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode column \"Claim Type\"\n",
    "\n",
    "replace_map = {\n",
    "    \"Claim Type\": {\n",
    "        \"Home Health Agency\": 1,\n",
    "        \"Hospice\": 2,\n",
    "        \"Inpatient\": 3,\n",
    "        \"Outpatient\": 4,\n",
    "        \"Skilled Nursing Facility\": 5,\n",
    "        \"Durable Medical Equipment\": 6,\n",
    "        \"Carrier\": 7,\n",
    "        \"Total\": 8,\n",
    "    }\n",
    "}\n",
    "table1.replace(replace_map, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column \"Percent of Spending Hospital\tPercent of Spending\" to float, remove the percent sign and\n",
    "# divide by 100 to normalize for percentage\n",
    "\n",
    "table1[\"Percent of Spending Hospital\"] = (\n",
    "    table1[\"Percent of Spending Hospital\"].str.rstrip(\"%\").astype(\"float\")\n",
    ")\n",
    "table1[\"Percent of Spending Hospital\"] = table1[\"Percent of Spending Hospital\"] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column \"Percent of Spending State\" to float, remove the percent sign and\n",
    "# divide by 100 to normalize for percentage\n",
    "\n",
    "table1[\"Percent of Spending State\"] = (\n",
    "    table1[\"Percent of Spending State\"].str.rstrip(\"%\").astype(\"float\")\n",
    ")\n",
    "table1[\"Percent of Spending State\"] = table1[\"Percent of Spending State\"] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column \"Percent of Spending Nation\" to float, remove the percent sign and\n",
    "# divide by 100 to normalize for percentage\n",
    "\n",
    "table1[\"Percent of Spending Nation\"] = (\n",
    "    table1[\"Percent of Spending Nation\"].str.rstrip(\"%\").astype(\"float\")\n",
    ")\n",
    "table1[\"Percent of Spending Nation\"] = table1[\"Percent of Spending Nation\"] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Column \"Facility Name\", Facility Id related to the facility, hence facility name is not\n",
    "# relevant for the model\n",
    "\n",
    "table1.drop([\"Facility Name\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the \"Avg Spending Per Episode Hospital\" column to the beginning, since the\n",
    "# algorithm requires the prediction column at the beginning\n",
    "\n",
    "col_name = \"Avg Spending Per Episode Hospital\"\n",
    "first_col = table1.pop(col_name)\n",
    "table1.insert(0, col_name, first_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert integer values to float in the columns \"Avg Spending Per Episode Hospital\",\n",
    "# \"Avg Spending Per Episode State\" and \"Avg Spending Per Episode Nation\"\n",
    "# Columns with integer values are interpreted as categorical values. Changing to float avoids any mis-interpretetaion\n",
    "\n",
    "table1[\"Avg Spending Per Episode Hospital\"] = table1[\n",
    "    \"Avg Spending Per Episode Hospital\"\n",
    "].astype(\"float\")\n",
    "table1[\"Avg Spending Per Episode State\"] = table1[\n",
    "    \"Avg Spending Per Episode State\"\n",
    "].astype(\"float\")\n",
    "table1[\"Avg Spending Per Episode Nation\"] = table1[\n",
    "    \"Avg Spending Per Episode Nation\"\n",
    "].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename long column names for costs and percentage costs on the hospital, state and nation,\n",
    "# so they are easily referenced in the rest of this discussion\n",
    "\n",
    "table1.rename(\n",
    "    columns={\n",
    "        \"Avg Spending Per Episode Hospital\": \"Avg_Hosp\",\n",
    "        \"Avg Spending Per Episode State\": \"Avg_State\",\n",
    "        \"Avg Spending Per Episode Nation\": \"Avg_Nation\",\n",
    "        \"Percent of Spending Hospital\": \"Percent_Hosp\",\n",
    "        \"Percent of Spending State\": \"Percent_State\",\n",
    "        \"Percent of Spending Nation\": \"Percent_Nation\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Start Date and End Date to datetime objects, then convert them to integers. First the data is converted\n",
    "# to Pandas datetime object. Then the year, month and days are extracted from the datetime object and\n",
    "# multipled with some weights to convert into final integer values.\n",
    "\n",
    "table1[\"Start Date\"] = pd.to_datetime(table1[\"Start Date\"])\n",
    "table1[\"End Date\"] = pd.to_datetime(table1[\"End Date\"])\n",
    "table1[\"Start Date\"] = (\n",
    "    1000 * table1[\"Start Date\"].dt.year\n",
    "    + 100 * table1[\"Start Date\"].dt.month\n",
    "    + table1[\"Start Date\"].dt.day\n",
    ")\n",
    "table1[\"End Date\"] = (\n",
    "    1000 * table1[\"End Date\"].dt.year\n",
    "    + 100 * table1[\"End Date\"].dt.month\n",
    "    + table1[\"End Date\"].dt.day\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the first 5 rows in the dataframe to see how the changed data looks\n",
    "\n",
    "table1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Columns \"Start Date\" and \"End Date\". The dataset is only for 2018, hence all start and end dates\n",
    "# are same in each row and does not impact the model\n",
    "\n",
    "table1.drop([\"Start Date\"], axis=1, inplace=True)\n",
    "table1.drop([\"End Date\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the table do not have missing values. The following code line shows there are no missing values\n",
    "# in the table\n",
    "\n",
    "table1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA):\n",
    "In this section, we perform **Exploratory Data Analysis** of the data set and use various techniques for feature selection\n",
    "\n",
    "First, we see the scatter_matrix plot of the feature variables in the data frame as they relate to the prediction variable `Avg_Hosp` cost. For this we use the scatter_matrix function from pandas.plotting library.\n",
    "\n",
    "The entire dataset has 67826 data rows. For analysis, we take a random sample of 400 data rows for the scatter_matrix. Before selecting the 400 random data rows, we use the scale function from sklearn.preprocessing library to appropriately scale the values of the data columns. This helps the scatter_matrix plot label decorations fit properly. This plots helps in determining if we should keep all the feature columns while training the model.\n",
    "\n",
    "Next we use the SelectKBest class and chi2 statistical test available from sklearn.feature_selection library to find the scores of feature columns as they relate to the prediction column. This is another mechanism to determine which feature columns are relevant to keep in the model\n",
    "\n",
    "Following that, we create and visualize the correlation matrix. This is another mechanism towards feature selection prior to training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After selecting the random sample of 400 data rows for the scatter_matrix analysis, this step\n",
    "# uses the scale function from sklearn.preprocessing library to scale the values. A new pandas data frame is created\n",
    "# that holds the sampled 400 data rows. We want to keep the original data set intact so we can use the original\n",
    "# data set for the subsequent training of the model\n",
    "\n",
    "table1_sample = table1.sample(n=400, random_state=2)\n",
    "standardised_table1_sample = scale(table1_sample)\n",
    "\n",
    "standardised_table1_sample = pd.DataFrame(\n",
    "    standardised_table1_sample, index=table1_sample.index, columns=table1_sample.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scatter_matrix is plotted using a tight layout for ease of visualization within the notebook. Hence,\n",
    "# in the scaled randomized sample of 400 data rows, the column names are renamed to shorter column names.\n",
    "\n",
    "standardised_table1_sample.rename(columns={\"Avg_Hosp\": \"A_Ho\"}, inplace=True)\n",
    "standardised_table1_sample.rename(columns={\"Facility ID\": \"F_Id\"}, inplace=True)\n",
    "standardised_table1_sample.rename(columns={\"State\": \"ST\"}, inplace=True)\n",
    "standardised_table1_sample.rename(columns={\"Period\": \"Per\"}, inplace=True)\n",
    "standardised_table1_sample.rename(columns={\"Claim Type\": \"Clm\"}, inplace=True)\n",
    "standardised_table1_sample.rename(columns={\"Avg_State\": \"A_ST\"}, inplace=True)\n",
    "standardised_table1_sample.rename(columns={\"Avg_Nation\": \"A_Na\"}, inplace=True)\n",
    "standardised_table1_sample.rename(columns={\"Percent_Hosp\": \"P_Ho\"}, inplace=True)\n",
    "standardised_table1_sample.rename(columns={\"Percent_State\": \"P_ST\"}, inplace=True)\n",
    "standardised_table1_sample.rename(columns={\"Percent_Nation\": \"P_Na\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, the scatter matrix is plotted between the prediction column \"Avg_Hosp\" whose shortened name is \"A_Ho\"\n",
    "# and each of the other remaining feature columns. For clarity of visualization, we create two scatter_matrix\n",
    "# plots. The first one showing Avg_Hosp with the columns \"Facility Id\", \"State\", \"Period\" and \"Claim Type\".\n",
    "# The second plot shows Avg_Hosp relation with the columns \"Avg_State\", \"Avg_Nation\", \"Percent_Hosp\",\n",
    "# \"Percent_State\" and \"Percent_Nation\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "pd.plotting.scatter_matrix(\n",
    "    standardised_table1_sample.loc[:, \"A_Ho\":\"Clm\"], diagonal=\"kde\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pd.plotting.scatter_matrix(\n",
    "    standardised_table1_sample.loc[:, [\"A_Ho\", \"A_ST\", \"A_Na\", \"P_Ho\", \"P_ST\", \"P_Na\"]],\n",
    "    diagonal=\"kde\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, we calculate the statistical scores of the feature columns as it relates to the prediction column\n",
    "# Avg_Hosp using the SelectKBest library function based on the chi2 statistical test. The scores are displayed in a\n",
    "# tabluar format for visualization. The X data frame has all the feature columns. The y data frame has the\n",
    "# prediction column.\n",
    "\n",
    "X = table1.iloc[:, 1:10]\n",
    "y = table1.iloc[:, 0:1]\n",
    "\n",
    "# We are selecting all the feature columns to see the scores for each feature column\n",
    "selected = SelectKBest(score_func=chi2, k=9)\n",
    "fit = selected.fit(X, y)\n",
    "datascores = pd.DataFrame(fit.scores_)\n",
    "datacolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "# concat two dataframes for better visualization\n",
    "featureScores = pd.concat([datacolumns, datascores], axis=1)\n",
    "featureScores.columns = [\"Features\", \"Score\"]  # naming the dataframe columns\n",
    "print(featureScores.nlargest(9, \"Score\"))  # print 9 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Correlation Matrix to see how the data is related\n",
    "\n",
    "corrMatrix = table1.corr()\n",
    "print(corrMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Correlation matrix with Searborn and Matplotlib\n",
    "plt.subplots(figsize=(15, 10))\n",
    "plt.tick_params(labelsize=14)\n",
    "sn.heatmap(corrMatrix, annot=True, annot_kws={\"size\": 12}, fmt=\".2f\", robust=True)\n",
    "plt.xlabel(\"Parameters\", fontsize=25)\n",
    "plt.ylabel(\"Parameters\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our target prediction is the column Avg_Hosp, we want to see\n",
    "# how this value is correlated with the other feature columns\n",
    "# From the below matrix, we see that the prediction column Avg_Hosp has the highest correlation\n",
    "# with the Avg_State feature column\n",
    "\n",
    "corrMatrix[\"Avg_Hosp\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train, Validation and Test Datasets:\n",
    "In the last step of the previous section, we see that the prediction column `Avg_Hosp` cost has second highest correlation with the `Avg_State` feature column. In this section we examine the spread of values for the `Avg_State`. Then we use a technique called stratification to categorize each data row based on the category of the `Avg_State` cost. We did not use the column `Percent_Hosp` (highest correlation) for stratification because the `Percent_Hosp` column is derived from the `Avg_Hosp` value and the line which contains the Total Cost per Episode for a specific hospital. There is an obvious expected high degree of correlation between `Avg_Hosp` and `Percent_Hosp`. Using `Percent_Hosp` to stratify values will not give us a good representative sample for train, validation and test datasets.\n",
    "\n",
    "From the defined categories of the `Avg_State` values, we use the StratifiedShuffleSplit function from the Scikit-Learn library to split and randomly select data subsets for the training, validation and test data sets. We use this method two times. First we split the original data into train and test. Then we split the test set again into validation and test set. This stratification technique allows us to have good representation of data in each of the train, validation and test sets that are well spread across the `Avg_State` value categories. Then we move the prediction column `Avg_Hosp` to the first column in each of the train, validation and test data sets. The data sets are uploaded into S3 bucket location during the machine learning model creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see the distribution of the data around the feature column \"Avg_State\" that has the highest\n",
    "# correlation to the target prediction \"Avg_Hosp\"\n",
    "# From the histogram below we see that most of the values are between 0 and 5000 for Avg_State\n",
    "\n",
    "sn.distplot(table1.Avg_State)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we divide this dataset into train, validation and test, we need to stratify the values of Avg_State\n",
    "# to ensure we effectively select random sets of data into the train, validation and test data sets\n",
    "# which is good representative sample based on the Avg_State values\n",
    "# To do this, we introduce a new Feature column column called \"Avg_State_Category\", we make this column\n",
    "# equal to the value of the respective Avg_State value divided by 1000 and then using the ceil (ceiling) function\n",
    "\n",
    "table1[\"Avg_State_Category\"] = np.ceil(table1[\"Avg_State\"] / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now see how these categories are distributed by aggregating across all the data rows\n",
    "\n",
    "table1.Avg_State_Category.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above distribution, we see approximately 61k out of the total 67k values have category 0.0 to 5.0\n",
    "# Only about 6k values of the total (approx 9%) of the values have category greater than 5.0\n",
    "# Hence we update the category to 6.0 for all data rows that have category greater than 5.0\n",
    "\n",
    "table1[\"Avg_State_Category\"].where(table1[\"Avg_State_Category\"] < 5, 6.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see the distribution of the values for the Avg_State_Category across the category values 0.0 to 6.0\n",
    "\n",
    "table1.Avg_State_Category.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the stratified shuffle split function available within sklearn library to create\n",
    "# train, validation and test datasets based on our defined perentages and splitting the data appropriately\n",
    "# and randomly across all the established categories with respect to the Avg_State values\n",
    "\n",
    "# The function splits the data into two parts. First we split into training set which will be 80% of the data\n",
    "# and a test set which will be 20% of the data. Then we split this test set into validation set\n",
    "# where the validation set will be 90% of the previous test set and the final test set is\n",
    "# the remaining 10% of the previous test set. This is a two fold splitting\n",
    "\n",
    "# First lets create the training set and the temporray set\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(table1, table1[\"Avg_State_Category\"]):\n",
    "    strat_train_set = table1.loc[train_index]\n",
    "    strat_test_set = table1.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take the test set and split it into the validation set and the test set\n",
    "# as mentioned in the prior step\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(\n",
    "    strat_test_set, strat_test_set[\"Avg_State_Category\"]\n",
    "):\n",
    "    strat_validation_set = table1.loc[train_index]\n",
    "    strat_test_set = table1.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total rows of data from the original dataset, training set, validation set and test set\n",
    "# To see the numbers match up, add the training, validation and test data set record counts to get the total in the\n",
    "# original data set\n",
    "\n",
    "Total = table1.shape[0]\n",
    "Train = strat_train_set.shape[0]\n",
    "Validation = strat_validation_set.shape[0]\n",
    "Test = strat_test_set.shape[0]\n",
    "Total, Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column \"Avg_State_category\" from train, validation and test datasets. This column\n",
    "# was introduced to do stratification of the data for meaningful sampling and is not relevant\n",
    "# for the model anymore\n",
    "\n",
    "strat_train_set.drop([\"Avg_State_Category\"], axis=1, inplace=True)\n",
    "strat_validation_set.drop([\"Avg_State_Category\"], axis=1, inplace=True)\n",
    "strat_test_set.drop([\"Avg_State_Category\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the bucket name with the your bucket name obtained form the CloudFormation output tab\n",
    "# From the SageMaker library, we use the get_execution_role function to get the execution\n",
    "# role for SageMaker to access AWS reqources while creating the machine learning model\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"linear_learner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each of the Train, Validation and Test datasets from pandas data frame into two subsets.\n",
    "# The x subset is the feature columns and the y subset is the label column\n",
    "\n",
    "x_train = strat_train_set.iloc[:, 1:10]\n",
    "y_train = strat_train_set.iloc[:, 0:1]\n",
    "\n",
    "x_validation = strat_validation_set.iloc[:, 1:10]\n",
    "y_validation = strat_validation_set.iloc[:, 0:1]\n",
    "\n",
    "x_test = strat_test_set.iloc[:, 1:10]\n",
    "y_test = strat_test_set.iloc[:, 0:1]\n",
    "\n",
    "train_df = pd.concat([y_train, x_train], axis=1)\n",
    "validation_df = pd.concat([y_validation, x_validation], axis=1)\n",
    "test_df = pd.concat([y_test, x_test], axis=1)\n",
    "\n",
    "# copy the training dataframe to s3\n",
    "train_df.to_csv(\"data/train_data.csv\", index=False, header=False)\n",
    "validation_df.to_csv(\"data/validation_data.csv\", index=False, header=False)\n",
    "test_df.to_csv(\"data/test_data.csv\", index=False, header=False)\n",
    "\n",
    "train_data_location = f\"s3://{bucket}/{prefix}/data/train\"\n",
    "validation_data_location = f\"s3://{bucket}/{prefix}/data/validation\"\n",
    "test_data_location = f\"s3://{bucket}/{prefix}/data/test\"\n",
    "\n",
    "# Upload the training data to S3\n",
    "S3Uploader.upload(\n",
    "    local_path=\"data/train_data.csv\",\n",
    "    desired_s3_uri=train_data_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# Upload the validation data to S3\n",
    "S3Uploader.upload(\n",
    "    local_path=\"data/validation_data.csv\",\n",
    "    desired_s3_uri=validation_data_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# Upload the testing data to S3\n",
    "S3Uploader.upload(\n",
    "    local_path=\"data/test_data.csv\",\n",
    "    desired_s3_uri=test_data_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "\n",
    "output_location = f\"s3://{bucket}/{prefix}/output\"\n",
    "\n",
    "train_data_location_input = sagemaker.inputs.TrainingInput(\n",
    "    train_data_location,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    record_wrapping=None,\n",
    "    compression=None,\n",
    ")\n",
    "\n",
    "validation_data_location_input = sagemaker.inputs.TrainingInput(\n",
    "    validation_data_location,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    record_wrapping=None,\n",
    "    compression=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Machine Learning Model using Amazon SageMaker:\n",
    "In this section, the SageMaker built-in Linear Learner algorithm is used to train the model using the training and validation data sets as input channels. The algorithm is used in the “Regressor” mode to train the model. The boto3 Python library for AWS and the SageMaker library for Python is used. In the step below, replace with your own **bucket name** from CloudFormation Outputs tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, we initialize the Linear Learner Estimator\n",
    "# We define the parameters for the estimator which will be used by SageMaker to train the model.\n",
    "# In this example, we use an instance of type \"ml.c4.xlarge\". Note that the SageMaker built-in Linear Learner\n",
    "# algorithm do not need GPU type instances mandatorily. GPU instances can be selected if the dataset is large\n",
    "# and using GPU instances will help boost the performance of the model creation process.\n",
    "\n",
    "# from sagemaker import LinearLearner\n",
    "# from sagemaker.sklearn.estimator import LinearLearner\n",
    "\n",
    "\n",
    "container = image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, framework=\"linear-learner\"\n",
    ")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    disable_profiler=True,\n",
    ")\n",
    "\n",
    "linear.set_hyperparameters(predictor_type=\"regressor\", mini_batch_size=200)\n",
    "# llearner = LinearLearner(role=role,\n",
    "#           predictor_type='regressor',\n",
    "#           normalize_data=True,\n",
    "#           normalize_label=True,\n",
    "#           instance_count=1,\n",
    "#           instance_type='ml.c5.xlarge',\n",
    "#           output_path=output_location,\n",
    "#           sagemaker_session=sagemaker_session,\n",
    "#           mini_batch_size=100\n",
    "#                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_location_input.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_location_input.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, we call the fit function to train the model using the training dataset and the validation dataset\n",
    "# llearner.fit([train_data_location,validation_data_location])\n",
    "linear.fit(\n",
    "    inputs={\n",
    "        \"train\": train_data_location_input,\n",
    "        \"validation\": validation_data_location_input,\n",
    "    },\n",
    "    logs=\"None\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, we deploy the model created in the previous step as an endpoint. In this example,\n",
    "# we use an instance type of \"ml.m4.xlarge\" to deploy the model. Once deployed, the endpoint\n",
    "# can be invoked to make inference and predict the value of the \"Avg_Hosp\" cost. Please note deploying the\n",
    "# model to an endpoint takes a few minutes\n",
    "\n",
    "llearner_predictor = linear.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m4.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llearner_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, we invoke the endpoint for a single inference. The first data row from the Test dataset is passed\n",
    "# to the endpoint for prediction. The predicted value is returned in the key value pair.\n",
    "\n",
    "# result = llearner_predictor.predict(x_test.values[0].astype('float32'))\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llearner_predictor.serializer = CSVSerializer()\n",
    "llearner_predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llearner_predictor.predict(\n",
    "    x_test.iloc()[0].tolist(), initial_args={\"ContentType\": \"text/csv\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = [\n",
    "    llearner_predictor.predict(\n",
    "        x_test.iloc()[i].tolist(), initial_args={\"ContentType\": \"text/csv\"}\n",
    "    )\n",
    "    for i in range(0, x_test.shape[0])\n",
    "]\n",
    "all_predictions_scores = [\n",
    "    all_predictions[i][\"predictions\"][0][\"score\"]\n",
    "    for i in range(0, len(all_predictions))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.pyplot.scatter(y_test[\"Avg_Hosp\"], all_predictions_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are finished testing, clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llearner_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We see how the SageMaker built-in Linear Learner algorithm is used to train machine learning model and use this model for inference. The steps show how Jupyter notebooks in SageMaker can be used for build, train and deployment of machine learning models and evaluation of metrics from the model’s performance. This approach can be used in a wide variety of use cases at scale."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
