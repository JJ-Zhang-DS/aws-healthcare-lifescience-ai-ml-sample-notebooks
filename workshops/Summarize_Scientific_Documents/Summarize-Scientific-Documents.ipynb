{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f42322-8e6b-4df5-9585-a969d364b12a",
   "metadata": {},
   "source": [
    "# Summarize Scientific Documents with Amazon Comprehend and HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e28edf-de16-4de0-96ff-c4b5678d4e17",
   "metadata": {},
   "source": [
    "Researchers must stay up-to-date on their fields of interest. However, it's difficult to keep track of the large number of journals, whitepapers, and research pre-prints generated in many areas. In response, many research groups have turned to AI/ML tools to summarize and classify new documents.\n",
    "\n",
    "In this workshop, we'll use several AWS AI/ML services to process scientific documents from the [NIH NCBI PMC Article Dataset](https://registry.opendata.aws/ncbi-pmc/) on the Registry of Open Data. This is a free full-text archive of biomedical and life sciences journal article at the U.S. National Institutes of Health's National Library of Medicine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da45f90-8e9c-4c0e-8fe6-da0f9bbe994b",
   "metadata": {},
   "source": [
    "# 1. Import Libraries and Create Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f953c1-9862-42b4-b195-6ad821562eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "import re\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "s3 = boto_session.client('s3')\n",
    "sm_session = sagemaker.Session(boto_session=boto_session)\n",
    "s3_bucket = sm_session.default_bucket()\n",
    "s3_prefix = \"sci-docs/data\"\n",
    "print(f\"S3 path is {s3_bucket}/{s3_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de31b09-df82-49af-876f-f5c76b4dd13c",
   "metadata": {},
   "source": [
    "# 2. Download Documents from the NIH NCBI PMC Article Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ac0c3-4b3d-48cb-84be-388f00f758ee",
   "metadata": {},
   "source": [
    "Copy 25 random articles from the PubMed open data set (https://registry.opendata.aws/ncbi-pmc/) into the SageMaker default bucket for this account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba4ce3-8f55-45b1-8acc-f0146e51a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmc_bucket = \"pmc-oa-opendata\"\n",
    "pmc_prefix = \"oa_comm/txt/all/\"\n",
    "local_raw_data_dir = \"data/raw/\"\n",
    "\n",
    "article_names = [ os.path.basename(article[\"Key\"]) for article in sample(s3.list_objects_v2(Bucket=pmc_bucket, Prefix=pmc_prefix)[\"Contents\"], 25) ]\n",
    "for article in article_names:\n",
    "    print(article)\n",
    "    sm_session.download_data(\n",
    "        local_raw_data_dir,\n",
    "        bucket=pmc_bucket,\n",
    "        key_prefix=pmc_prefix+article\n",
    "    )    \n",
    "    \n",
    "# Once all files have been downloaded, upload them all to the S3 bucket for your project\n",
    "sm_session.upload_data(\n",
    "    local_raw_data_dir,\n",
    "    bucket=s3_bucket,\n",
    "    key_prefix=s3_prefix+\"/raw\"\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c3891-39c2-4db0-ad41-897c802971f9",
   "metadata": {},
   "source": [
    "Look at a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08584a-adb6-4988-8186-9f42a2387527",
   "metadata": {},
   "outputs": [],
   "source": [
    "art = sample(article_names,1)[0]\n",
    "print(art)\n",
    "!head data/raw/{art}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5548da-8719-43a7-9480-9efda9f5fecc",
   "metadata": {},
   "source": [
    "# 3. Summarize the Documents Using Amazon Comprehend Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bed8d3-6fe9-44cc-9e1a-f9b7951f0b74",
   "metadata": {},
   "source": [
    "Submit an Amazon Comprehend topic modelling job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984d622-2a9f-42db-9d70-29c25c941d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto_session.client(service_name='comprehend')\n",
    "\n",
    "sagemaker.s3.s3_path_join(s3_bucket, s3_prefix, \"raw\")\n",
    "\n",
    "input_s3_url = sagemaker.s3.s3_path_join(\"s3://\", s3_bucket, s3_prefix, \"raw\")\n",
    "input_doc_format = \"ONE_DOC_PER_FILE\"\n",
    "output_s3_url = sagemaker.s3.s3_path_join(\"s3://\", s3_bucket, s3_prefix, \"output\")\n",
    "data_access_role_arn = sagemaker.session.get_execution_role()\n",
    "number_of_topics = 25\n",
    "\n",
    "input_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\n",
    "output_data_config = {\"S3Uri\": output_s3_url}\n",
    "\n",
    "start_topics_detection_job_result = comprehend.start_topics_detection_job(NumberOfTopics=number_of_topics,\n",
    "                                                                              InputDataConfig=input_data_config,\n",
    "                                                                              OutputDataConfig=output_data_config,\n",
    "                                                                              DataAccessRoleArn=data_access_role_arn)\n",
    "\n",
    "job_id = start_topics_detection_job_result[\"JobId\"]\n",
    "print(f\"Job {job_id} submitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea15645d-efa1-48e1-b666-0c819fdb865e",
   "metadata": {},
   "source": [
    "Once job is finished, download and unpack the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63594d-d585-424c-8150-b017066b4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_topics_detection_job_result = comprehend.describe_topics_detection_job(JobId=job_id).get(\"TopicsDetectionJobProperties\", [])\n",
    "print(f\"Job {job_id} status is {describe_topics_detection_job_result['JobStatus']}\")\n",
    "\n",
    "if describe_topics_detection_job_result[\"JobStatus\"] == \"COMPLETED\":\n",
    "    output_url = sagemaker.s3.parse_s3_url(describe_topics_detection_job_result[\"OutputDataConfig\"][\"S3Uri\"])\n",
    "    sm_session.download_data(\n",
    "        \"data\",\n",
    "        bucket=output_url[0],\n",
    "        key_prefix=output_url[1],\n",
    "    )\n",
    "    os.system(\"!tar xvfz data/output.tar.gz -C data/output/ && rm data/output.tar.gz\")\n",
    "\n",
    "    topics = pd.read_csv(\"data/topic-terms.csv\").sort_values(['topic', 'weight'], ascending=[True, False]).groupby(['topic'])['term'].agg(lambda x : ', '.join(x))\n",
    "    docs = pd.read_csv(\"data/doc-topics.csv\").sort_values(['docname', 'proportion'], ascending=[True, False])\n",
    "    results = pd.merge(docs, topics, how='left', on='topic')\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcae0b6-c17a-4304-a448-10f94d6fc3cc",
   "metadata": {},
   "source": [
    "Let's look at some specific examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f4ddb-1fc9-4fc7-9df0-6ab9e209288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if describe_topics_detection_job_result[\"JobStatus\"] == \"COMPLETED\":\n",
    "\n",
    "    input_url = sagemaker.s3.parse_s3_url(describe_topics_detection_job_result[\"InputDataConfig\"][\"S3Uri\"])\n",
    "    sample = results.sample()\n",
    "    docname, idx, score, terms = sample.iloc[0,:]\n",
    "\n",
    "    print(f\"Document name is {docname}\")\n",
    "    print(f\"Identified terms are {terms}\")\n",
    "\n",
    "    sm_session.download_data(\n",
    "            \"data\",\n",
    "            bucket=input_url[0],\n",
    "            key_prefix=os.path.join(input_url[1], docname)\n",
    "        )\n",
    "    os.system(f\"head -n 25 data/{docname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80681160-180a-4fce-8ce1-cc79899aeae0",
   "metadata": {},
   "source": [
    "# 4. Generate TLDR Summaries Using a Pre-Trained NLP Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785636a6-6033-469c-b3fb-7522fc6248ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n",
    "from sagemaker.async_inference.waiter_config import WaiterConfig\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'alk/pegasus-scitldr',\n",
    "\t'HF_TASK':'text2text-generation'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model= HuggingFaceModel(\n",
    "\ttransformers_version='4.17.0',\n",
    "\tpytorch_version='1.10.2',\n",
    "\tpy_version='py38',\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d918044-8196-43cf-8f53-214985758539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{s3_bucket}/{s3_prefix}/tldr_output\",\n",
    "    max_concurrent_invocations_per_instance=4\n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "    async_inference_config=async_config,\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.4xlarge', # ec2 instance type\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a92d55-9592-44d0-b6fe-b8cbdf782c9d",
   "metadata": {},
   "source": [
    "Convert document text to json format and upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88eb3e2-7f70-41e1-aa95-7efa53f5360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find an article with well-defined background information\n",
    "result = None\n",
    "while result is None:\n",
    "    art = sample(article_names,1)[0]\n",
    "    print(art)\n",
    "\n",
    "    with open(f\"data/raw/{art}\", \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        text = f.read().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        result = re.search(\"Background (.{,1000})\", text)\n",
    "\n",
    "dict = {\"inputs\": result.group(1)} # Search for background infomation\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b51c0-8e4f-4043-8efd-f966e1452a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_response = predictor.predict_async(data=dict)\n",
    "\n",
    "waiter = WaiterConfig(max_attempts=24, delay=15)\n",
    "result = async_response.get_result(waiter)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b4a91-0422-43f5-81e6-8274bb077115",
   "metadata": {},
   "source": [
    "# 5. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ed5ce-5baa-4cb1-be43-02b4b9f913db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "\n",
    "# Delete all S3 objects\n",
    "bucket = boto_session.resource(\"s3\").Bucket(s3_bucket)\n",
    "bucket.objects.filter(Prefix=\"sci-docs\").delete()\n",
    "os.system(f\"rm -rf data\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
