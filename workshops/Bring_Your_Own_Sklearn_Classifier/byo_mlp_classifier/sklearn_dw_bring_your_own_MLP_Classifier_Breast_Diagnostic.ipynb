{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Whether a Breast Cancer Sample is Benign or Malignant\n",
    "\n",
    "*This notebooks extends [sklearn_bring_your_own_MLP_Classifier_Breast_Diagnostic.ipynb](https://github.com/aws-samples/aws-healthcare-lifescience-ai-ml-sample-notebooks/blob/main/workshops/Bring_Your_Own_Sklearn_Classifier/byo_mlp_classifier/sklearn_bring_your_own_MLP_Classifier_Breast_Diagnostic.ipynb) by using SageMaker Data Wrangler.*\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "\n",
    "1. Understand what SageMaker Script Mode is, and how it can be leveraged.\n",
    "1. Use SageMaker Data Wrangler data for feature engineering.\n",
    "1. Use prebuilt SageMaker containers to build, train, and deploy customer sklearn model.\n",
    "1. Use batch transform to perform inferences and measure model performance.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This is a breast cancer diagnoses dataset, where, for each sample, the sample is diagnosed as \"Benign\" or \"Malignant\". For each sample, a number of features are given as well. The source of the dataset is the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).\n",
    "\n",
    "For this model, we will build, train and deploy a [Multi-layer Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) using the sklearn library.\n",
    "\n",
    "To prepare data, we are going to use [SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/).\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "1. [Process Data](#Process-Data)\n",
    "    1. [Prepare dataset](#Prepare-dataset)\n",
    "    1. [Option 1. Create a workflow using Data Wrangler](#Option-1.-Create-a-workflow-using-Data-Wrangler)\n",
    "    1. [Option 2. Use existing Data Wrangler workflow](#Option-2.-Use-existing-Data-Wrangler-workflow)\n",
    "    1. [Export to run as a SageMaker Pipeline](#Export-to-run-as-a-SageMaker-Pipeline)\n",
    "    1. [Run Processing job](#Run-Processing-job)\n",
    "    1. [Prepare data for training and testing](#Prepare-data-for-training-and-testing)\n",
    "1. [Train the Model](#Train-the-Model)\n",
    "1. [Make Batch Predictions](#Make-Batch-Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have the right version of sagemaker\n",
    "%pip install sagemaker==2.72.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and initialize necessary clients\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sklearn import model_selection\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sagemaker_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "BUCKET = sagemaker_session.default_bucket()\n",
    "PREFIX = \"breast_cancer\"\n",
    "PREFIX_RAW = PREFIX + '/raw'\n",
    "PREFIX_PROCESSED = PREFIX + '/processed'\n",
    "\n",
    "# Initialize dataset URL\n",
    "original_s3_dataset_url = 's3://sagemaker-sample-files/datasets/tabular/breast_cancer/wdbc.csv'\n",
    "original_s3_dataset_region = 'us-east-1'\n",
    "\n",
    "# Existing Data Wrangler flow file\n",
    "hcls_lab_flow_file = 'hcls-lab.flow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Processing job expects S3 inputs to be in the same region. If you are not running notebook from `us-east-1`, code below will created a new bucket, and copy public data set to a private bucket in the region of your SageMaker domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "s3_dataset_url = ''\n",
    "\n",
    "if region != original_s3_dataset_region:\n",
    "    s3_dataset_path = original_s3_dataset_url[4:]\n",
    "    s3_dataset_filename = original_s3_dataset_url.split('/')[-1]\n",
    "\n",
    "    response = s3_client.copy_object(\n",
    "        Bucket=BUCKET,\n",
    "        CopySource=s3_dataset_path,\n",
    "        Key=f'{PREFIX_RAW}/{s3_dataset_filename}',\n",
    "    )\n",
    "    \n",
    "    s3_dataset_url = f's3://{BUCKET}/{PREFIX_RAW}/{s3_dataset_filename}'\n",
    "    print(f'Copied public dataset to private bucket in region={region}:')\n",
    "else:\n",
    "    s3_dataset_url = original_s3_dataset_url\n",
    "    print(f'S3 bucket of dataset matches SageMaker domain region={region} and can be used as is:')\n",
    "    \n",
    "print(s3_dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1. Create a workflow using Data Wrangler\n",
    "\n",
    "You can follow steps below to create a Data Wrangler flow. Or you skip these steps and use existing flow by following [Option 2. Use existing Data Wrangler workflow](#Option-2.-Use-existing-Data-Wrangler-workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create new Data Wrangler flow\n",
    "\n",
    "![Create Flow](./images/dw_create_flow.png \"Create Flow\")\n",
    "\n",
    "Rename flow. You can choose any name, for example `hcsl-lab-2022`.\n",
    "\n",
    "![Rename Flow](./images/dw_rename_flow.png \"Rename Flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Configure data import from S3\n",
    "\n",
    "![S3 Import](./images/dw_import_s3.png \"S3 Import\")\n",
    "\n",
    "Configure and start S3 Import.\n",
    "\n",
    "*If your SageMaker Studio domain is in `us-east-1`, you can use a link to a public data set: `s3://sagemaker-sample-files/datasets/tabular/breast_cancer/wdbc.csv`. Otherwise, use a copy of this file, created in [Prepare dataset](#Prepare-dataset) section.*\n",
    "\n",
    "![Configure and start S3 Import](./images/dw_import_s3_start.png \"Configure and start S3 Import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Add Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Add Transform\"](./images/dw_transform.png \"Add Transform\")\n",
    "\n",
    "Add New Transformation.\n",
    "\n",
    "![\"Add New Transform\"](./images/dw_transform_add.png \"Add New Transform\")\n",
    "\n",
    "Add Custom transformation to add columns names.\n",
    "\n",
    "![\"Add Transform - Custom\"](./images/dw_transform_custom.png \"Add Transform - Custom\")\n",
    "\n",
    "Select Pandas:\n",
    "\n",
    "![\"Add Transform - Custom Pandas\"](./images/dw_transform_pandas.png \"Add Transform - Custom Pandas\")\n",
    "\n",
    "And paste this code:\n",
    "\n",
    "```python\n",
    "column_names=[\n",
    "        \"id\",\n",
    "        \"diagnosis\",\n",
    "        \"radius_mean\",\n",
    "        \"texture_mean\",\n",
    "        \"perimeter_mean\",\n",
    "        \"area_mean\",\n",
    "        \"smoothness_mean\",\n",
    "        \"compactness_mean\",\n",
    "        \"concavity_mean\",\n",
    "        \"concave points_mean\",\n",
    "        \"symmetry_mean\",\n",
    "        \"fractal_dimension_mean\",\n",
    "        \"radius_se\",\n",
    "        \"texture_se\",\n",
    "        \"perimeter_se\",\n",
    "        \"area_se\",\n",
    "        \"smoothness_se\",\n",
    "        \"compactness_se\",\n",
    "        \"concavity_se\",\n",
    "        \"concave points_se\",\n",
    "        \"symmetry_se\",\n",
    "        \"fractal_dimension_se\",\n",
    "        \"radius_worst\",\n",
    "        \"texture_worst\",\n",
    "        \"perimeter_worst\",\n",
    "        \"area_worst\",\n",
    "        \"smoothness_worst\",\n",
    "        \"compactness_worst\",\n",
    "        \"concavity_worst\",\n",
    "        \"concave points_worst\",\n",
    "        \"symmetry_worst\",\n",
    "        \"fractal_dimension_worst\",\n",
    "    ]\n",
    "\n",
    "df.columns = column_names\n",
    "```\n",
    "\n",
    "Add \"Manage Columns\" transform and select \"Drop column\" transform for \"id\" column.\n",
    "\n",
    "![\"Add Transform - Drop id\"](./images/dw_transform_drop_column_id.png \"Add Transform - Drop id\")\n",
    "\n",
    "Add \"Encode Categorical\" transform, choose \"One-hot encode\" for \"diagnosis\" column and select \"Columns\" for Output style.\n",
    "\n",
    "![\"Add Transform - One-hot encode for diagnosis\"](./images/dw_transform_encode_categorical.png \"Add Transform - One-hot encode for diagnosis\")\n",
    "\n",
    "Add \"Manage Columns\" transform and select \"Drop column\" transform for generated above \"diagnosis_B\" column.\n",
    "\n",
    "![\"Add Transform - Drop diagnosis_B\"](./images/dw_transform_drop_column_diagnosisb.png \"Add Transform - Drop diagnosis_B\")\n",
    "\n",
    "Add \"Manage Columns\" transform and select \"Rename column\" transform for \"diagnosis_M\" column; set \"truth\" as a new name.\n",
    "\n",
    "![\"Add Transform - Rename diagnosis_M\"](./images/dw_transform_rename.png \"Add Transform - Rename diagnosis_M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Use existing Data Wrangler workflow\n",
    "\n",
    "To continue, we will use existing flow from `hcls-lab.flow` file. If you want to start with creating a new flow, you can follow steps in [Option 1. Create a workflow using Data Wrangler](#Option-1.-Create-a-workflow-using-Data-Wrangler).\n",
    "\n",
    "Code below depends on [Copy dataset (if your SM domain not in us-east-1)](#Copy-dataset-(if-your-SM-domain-not-in-us-east-1)), so if you skipped it before, now it's time to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hcls_lab_flow_file, \"r\") as fr:\n",
    "    flow_data = json.load(fr)\n",
    "    source_node = list(filter(lambda n: n['type'] == 'SOURCE', flow_data['nodes']))[0]\n",
    "    source_node['parameters']['dataset_definition']['s3ExecutionContext']['s3Uri'] = s3_dataset_url\n",
    "                      \n",
    "with open(hcls_lab_flow_file, \"w\") as fw:\n",
    "    json.dump(flow_data, fw, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can open existing Data Wrangler flow and view all transformations. To do this navigate to \"File Browser\" of SageMaker Studio UI and open `hcls-lab.flow` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to run as a SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Configure Export\n",
    "![Configure Export](./images/dw_export.png \"Configure export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Start Export\n",
    "![Start Export](./images/dw_export_start.png \"Start export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Update generated notebook\n",
    "Open generated notebook and modify the code in `Output: S3 settings` section:\n",
    "\n",
    "Code before change:\n",
    "```python\n",
    "flow_export_name = f\"flow-{flow_export_id}\"\n",
    "```\n",
    "\n",
    "Code after change:\n",
    "```python\n",
    "flow_export_name = f\"hcls-lab-flow-{flow_export_id}\"\n",
    "```\n",
    "\n",
    "We will use this string prefix as a filter to search for a pipeline execution and its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Processing job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Execute notebook code - \"Run all cells\" from a main menu.\n",
    "1. Wait until execution finishes (~5 minutes).\n",
    "1. Check status in Pipelines section of SageMaker resources: open \"SageMaker resources\" tab, select \"Pipelines\" and open recently created pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training and testing\n",
    "\n",
    "SageMaker Data Wrangler saves data to one output location. Our training and prediction code expects distinct data sets.\n",
    "\n",
    "Now we will load processed data from S3, split it into training and test data sets and save to separate S3 locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Processed Data from S3\n",
    "\n",
    "Below we are going to:\n",
    "1. Get definition of a most recent pipeline, matching prefix defined above (`hcls-lab`).\n",
    "1. Get a most recent pipeline execution.\n",
    "1. Get S3 prefix for pipeline execution and all created S3 output objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_prefix = 'hcls-lab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent pipeline defintion matching given prefix.\n",
    "pipelines_list = sagemaker_client.list_pipelines()\n",
    "pipeline_name = list(filter(lambda pipeline: pipeline_prefix in pipeline['PipelineName'], pipelines_list['PipelineSummaries']))[0]['PipelineName']\n",
    "\n",
    "response = sagemaker_client.describe_pipeline(\n",
    "    PipelineName=pipeline_name\n",
    ")\n",
    "\n",
    "definition = json.loads(response['PipelineDefinition'])\n",
    "dw_processing_step = definition['Steps'][0]\n",
    "print(json.dumps(dw_processing_step, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List executions of this pipeline and get the most recent one.\n",
    "response = sagemaker_client.list_pipeline_executions(\n",
    "    PipelineName=pipeline_name\n",
    ")\n",
    "\n",
    "execution_arn = response['PipelineExecutionSummaries'][0]['PipelineExecutionArn']\n",
    "print(execution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output S3 prefix for the most recent pipeline execution.\n",
    "from sagemaker.processing import ProcessingJob\n",
    "\n",
    "response = sagemaker_client.list_pipeline_execution_steps(\n",
    "    PipelineExecutionArn=execution_arn\n",
    ")\n",
    "\n",
    "execution_prefix = ProcessingJob.from_processing_arn(sagemaker_session, response['PipelineExecutionSteps'][0]['Metadata']['ProcessingJob']['Arn']).describe()[\"ProcessingJobName\"]\n",
    "print(execution_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble complete S3 prefix for the execution output.\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "dw_s3_output = dw_processing_step['Arguments']['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "parse_result = urlparse(dw_s3_output)\n",
    "\n",
    "dw_s3_output_bucket=parse_result.netloc\n",
    "dw_s3_output_prefix=f'{parse_result.path[1:]}/{execution_prefix}'\n",
    "\n",
    "print(f'dw_s3_output_bucket: {dw_s3_output_bucket}')\n",
    "print(f'dw_s3_output_prefix: {dw_s3_output_prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all S3 objects for the output prefix and get the most recent file\n",
    "response = s3_client.list_objects(Bucket=dw_s3_output_bucket, Prefix=dw_s3_output_prefix)\n",
    "dw_s3_output_key=response['Contents'][-1]['Key']\n",
    "\n",
    "full_data_path=f's3://{dw_s3_output_bucket}/{dw_s3_output_key}'\n",
    "full_data_file=full_data_path.split('/')[-1]\n",
    "\n",
    "print(f'full_data_path: {full_data_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3Downloader.download(\n",
    "    s3_uri=full_data_path,\n",
    "    local_path=\"data\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "df_data = pandas.read_csv(\"data/\" + full_data_file)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data sets into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training (70%) and test (30%) sets\n",
    "train_df, test_df = model_selection.train_test_split(df_data, test_size=0.3)\n",
    "print(f\"The train data has shape {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test data set\n",
    "y_test = test_df[\"truth\"].tolist()\n",
    "x_test = test_df.drop([\"truth\"], axis=1)\n",
    "\n",
    "print(f\"The test data has shape {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the training and test data sets to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the training data to s3 so that sagemaker can read it\n",
    "train_df.to_csv(\"data/train_data.csv\", index=False)\n",
    "training_data_path = S3Uploader.upload(\n",
    "    local_path=\"data/train_data.csv\",\n",
    "    desired_s3_uri=f\"s3://{BUCKET}/{PREFIX_PROCESSED}\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# Do the same for the test data\n",
    "x_test.to_csv(\"data/x_test.csv\", index=False, header=False)\n",
    "test_data_path = S3Uploader.upload(\n",
    "    local_path=\"data/x_test.csv\",\n",
    "    desired_s3_uri=f\"s3://{BUCKET}/{PREFIX_PROCESSED}\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\"))\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    train_data=pd.read_csv(os.path.join(args.train, \"train_data.csv\"))\n",
    "\n",
    "    # Extract the labels from the first column\n",
    "    train_y = train_data[\"truth\"]\n",
    "    train_X = train_data.drop([\"truth\"], axis=1)\n",
    "\n",
    "    # Use scikit-learn's MLP Classifier to train the model.\n",
    "    regr = MLPClassifier(random_state=1, max_iter=500).fit(train_X, train_y)\n",
    "    regr.get_params()\n",
    "\n",
    "    # Print the coefficients of the trained classifier, and save the coefficients\n",
    "    joblib.dump(regr, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    regr = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return regr\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"return the class and the probability of the class\"\"\"\n",
    "    prediction = model.predict(input_data)\n",
    "    pred_prob = model.predict_proba(input_data) # A numpy array\n",
    "    return np.array(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the estimator\n",
    "sklearn = SKLearn(\n",
    "    entry_point=\"train.py\",\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    role=role,\n",
    "    py_version=\"py3\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off the training job, it takes ~5 minutes\n",
    "sklearn.fit({\"train\": training_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a batch transformer for predictions\n",
    "transformer = sklearn.transformer(\n",
    "    instance_count=1, instance_type=\"ml.m4.xlarge\", accept=\"text/csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a transform job and wait for it to finish\n",
    "batch_input_s3 = test_data_path\n",
    "transformer.transform(batch_input_s3, content_type=\"text/csv\", split_type=\"Line\")\n",
    "print(\"Waiting for transform job: \" + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the output data from S3 to local filesystem\n",
    "batch_output = transformer.output_path\n",
    "print(f\"Batch transform results saved to {batch_output}\")\n",
    "S3Downloader.download(\n",
    "    s3_uri=batch_output,\n",
    "    local_path=\"data/output\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the batch transform results\n",
    "!head data/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictions and measure performance\n",
    "predictions = pandas.read_csv(\"data/output/x_test.csv.out\", header=None)\n",
    "predictions.reset_index(drop=True, inplace=True)\n",
    "results = pandas.concat([predictions, pandas.Series(y_test)], axis=1)\n",
    "results.columns = [\"pred_0\", \"pred_1\", \"true\"]\n",
    "results[\"true\"] = results[\"true\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, threshold = metrics.roc_curve(results[\"true\"], results[\"pred_1\"])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we used SageMaker script mode to build, train, and deploy a sklearn model.\n",
    "Also, we used SageMaker Data Wrangler for feature engineering.\n",
    "\n",
    "\n",
    "The Jupyter Notebook that was produced by Data Wrangler during exported, was used to define a pipeline. The pipeline included a data processing step that was defined by data flow. There are other ways to export Data Wrangler flows:\n",
    "\n",
    "* [Python code](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-export.html#data-wrangler-data-export-python-code) - a Python file is created containing all steps in your data flow.\n",
    "* [Feature Store](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-export.html#data-wrangler-data-export-feature-store) - Jupyter Notebook processes your dataset using a SageMaker Data Wrangler job, and then ingests the data into an online and offline feature store."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
