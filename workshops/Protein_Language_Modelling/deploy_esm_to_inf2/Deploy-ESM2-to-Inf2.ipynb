{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ab7591-1443-4854-881a-f0d37a7ad1d6",
   "metadata": {},
   "source": [
    "# Deploy pre-trained ESM-2 model to Inferentia2\n",
    "\n",
    "Note: This notebook was last tested in SageMaker Studio on the PyTorch 1.13 Python 3.9 CPU Optimized image on a ml.c5.4xlarge instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f5ab1-119a-482d-9377-84cc8088fec9",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Install neuronx and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07df43-b111-4625-87ff-3519e351b5ff",
   "metadata": {},
   "source": [
    "Install the neuronx compiler. NOTE: You will need to restart your notebook kernel after running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b670bb6-2414-4b1a-81fa-213e21945c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "apt-get update -y\n",
    "apt-get install gpg-agent -y\n",
    "wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | apt-key add -\n",
    "add-apt-repository https://apt.repos.neuron.amazonaws.com\n",
    "apt-get update -y\n",
    "apt-get install aws-neuronx-dkms=2.* aws-neuronx-collectives=2.* aws-neuronx-runtime-lib=2.* aws-neuronx-tools=2.* -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981be8bf-4e67-40fd-a900-6e6319ec1227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q --upgrade pip\n",
    "%pip install -q --upgrade --extra-index-url https://pip.repos.neuron.amazonaws.com \\\n",
    "  neuronx-cc==2.* torch sagemaker boto3 awscli transformers accelerate boto3 datasets \\\n",
    "  torch-neuronx=='1.13.1.1.10.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025fc48-3c08-4f2e-bee0-326846d9e94c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto_session = boto3.session.Session(profile_name=None, region_name=None)\n",
    "sagemaker_session = sagemaker.session.Session(boto_session)\n",
    "S3_BUCKET = sagemaker_session.default_bucket()\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_execution_role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "print(f\"Assumed SageMaker role is {sagemaker_execution_role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc16165-cb4b-4c31-a279-0e13f31d6552",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 2. Compile pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdde14f-e621-4d0c-9a5c-267945b2c353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# MODEL_ID=\"facebook/esm2_t33_650M_UR50D\"\n",
    "# MODEL_ID=\"facebook/esm2_t12_35M_UR50D\"\n",
    "MODEL_ID = \"facebook/esm2_t6_8M_UR50D\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_ID, torchscript=True)\n",
    "model.eval()\n",
    "\n",
    "sequence = \"QVQLVESGGGVVQPRSLTLSCAASGFTFSSYGL<mask>HWVRQAPGKGLEWVANIWYDGANKYYGDSVKGRFTISRDNSRNTLYLQMNSLTAEDTAVYYCARWIEYGSGKDAFDVWGQGTMVIVSS\"\n",
    "max_length = 128\n",
    "tokenized_sequence = tokenizer.encode_plus(\n",
    "    sequence,\n",
    "    max_length=max_length,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "tracing_input = tokenized_sequence[\"input_ids\"], tokenized_sequence[\"attention_mask\"]\n",
    "\n",
    "print(\"Testing model inference\")\n",
    "print(model(*tracing_input)[0])\n",
    "\n",
    "\n",
    "print(\"Beginning model trace\")\n",
    "model_trace_start_time = timer()\n",
    "neuron_model = torch_neuronx.trace(model, tracing_input)\n",
    "neuron_model.save(\"traced_esm.pt\")\n",
    "print(f\"Model trace completed in {round(timer() - model_trace_start_time, 0)} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428b1f3-5b74-49fe-aa01-b2ce38d538b5",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Assemble model package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7cfaf-e7fb-4910-b3e2-6c21813d6560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -czvf model.tar.gz traced_esm.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92364e-2aab-4a01-b917-2a83fefbc1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S3_PREFIX = \"compiled-model\"\n",
    "S3_PATH = sagemaker.s3.s3_path_join(\"s3://\", S3_BUCKET, S3_PREFIX)\n",
    "print(f\"S3 path is {S3_PATH}\")\n",
    "s3_model_uri = sagemaker_session.upload_data(\"model.tar.gz\", S3_BUCKET, S3_PREFIX)\n",
    "print(f\"Model artifact uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c91e85-e9d9-465c-9681-eb9d1797dca3",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Define inference script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17a2a0-8e61-485d-bb84-6dc4b0c7419f",
   "metadata": {},
   "source": [
    "We define the code needed to load our neuron-compiled model in a `inference.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca6f59-d163-404d-961a-ad56e21b5741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/requirements.txt\n",
    "--extra-index-url=https://pip.repos.neuron.amazonaws.com\n",
    "transformers\n",
    "torch-neuronx==1.13.1.1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1e5c6-b9fa-4780-943f-3a0ff58b77ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/inference.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "JSON_CONTENT_TYPE = \"application/json\"\n",
    "# MODEL_ID = \"facebook/esm2_t33_650M_UR50D\"\n",
    "# MODEL_ID = \"facebook/esm2_t12_35M_UR50D\"\n",
    "MODEL_ID = \"facebook/esm2_t6_8M_UR50D\"\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model from HuggingFace\"\"\"\n",
    "    print(f\"torch-neuronx version is {torch_neuronx.__version__}\")\n",
    "    tokenizer_init = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    model_file = os.path.join(model_dir, \"traced_esm.pt\")\n",
    "    neuron_model = torch.jit.load(model_file)\n",
    "    return (neuron_model, tokenizer_init)\n",
    "\n",
    "def input_fn(serialized_input_data, content_type=JSON_CONTENT_TYPE):\n",
    "    \"\"\" Process the request payload \"\"\"\n",
    "    \n",
    "    if content_type == JSON_CONTENT_TYPE:\n",
    "        input_data = json.loads(serialized_input_data)\n",
    "        return input_data.pop(\"inputs\", input_data)\n",
    "    else:\n",
    "        raise Exception(\"Requested unsupported ContentType in Accept: \" + content_type)\n",
    "        return\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model_and_tokenizer):\n",
    "    \"\"\" Run model inference \"\"\"\n",
    "    \n",
    "    model_bert, tokenizer = model_and_tokenizer\n",
    "    max_length = 128\n",
    "    tokenized_sequence = tokenizer.encode_plus(\n",
    "        input_data,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    prediction_input = (\n",
    "        tokenized_sequence[\"input_ids\"],\n",
    "        tokenized_sequence[\"attention_mask\"],\n",
    "    )\n",
    "    output = neuron_model(*prediction_input)[0]\n",
    "    mask_token_index = (tokenized_sequence.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "    mask_index_predictions = output[0, mask_token_index]\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(mask_index_predictions)\n",
    "    return {\n",
    "        list(tokenizer.get_vocab().keys())[idx]: round(v.item(), 3)\n",
    "        for idx, v in enumerate(probs[0])\n",
    "    }\n",
    "\n",
    "\n",
    "def output_fn(prediction_output, accept=JSON_CONTENT_TYPE):\n",
    "    \"\"\" Process the response payload \"\"\"\n",
    "    if accept == JSON_CONTENT_TYPE:\n",
    "        return json.dumps(prediction_output), accept\n",
    "\n",
    "    raise Exception(\"Requested unsupported ContentType in Accept: \" + accept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821c118-fe6b-45f1-903d-c90eaae929fd",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Deploy inf2 model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34550d39-06fb-489b-ace8-375186985dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "ecr_image = f\"763104351884.dkr.ecr.{sagemaker_session.boto_region_name}.amazonaws.com/pytorch-inference-neuronx:1.13.0-neuronx-py38-sdk2.9.0-ubuntu20.04\"\n",
    "\n",
    "inf2_model = PyTorchModel(\n",
    "    model_data=s3_model_uri,\n",
    "    role=sagemaker_execution_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=ecr_image,\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model via neuron-cc\n",
    "inf2_model._is_compiled_model = True\n",
    "\n",
    "inf2_predictor = inf2_model.deploy(\n",
    "    instance_type=\"ml.inf2.xlarge\", initial_instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0e16e-59a0-4c51-bea5-af159e60cd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# ecr_image = f\"763104351884.dkr.ecr.{sagemaker_session.boto_region_name}.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13.0-transformers4.28.1-neuronx-py38-sdk2.9.1-ubuntu20.04\"\n",
    "# inf2_model = HuggingFaceModel(\n",
    "#     model_data = s3_model_uri,\n",
    "#     role=sagemaker_execution_role,\n",
    "#     source_dir=\"scripts\",\n",
    "#     entry_point=\"inference.py\",\n",
    "#     image_uri=ecr_image\n",
    "# )\n",
    "# inf2_model._is_compiled_model = True\n",
    "\n",
    "# inf2_predictor = inf2_model.deploy(\n",
    "#     instance_type=\"ml.inf2.xlarge\", initial_instance_count=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c23e55-c855-4f49-8ca9-d980e8c72a12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = \"QVQLVESGGGVVQ<mask>PGRSLTLSCAASGFTFSSYGLHWVRQAPGKGLE\"\n",
    "inf2_predictor.predict({\"inputs\": example})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371901a-5d20-425a-9925-3b885d9a4794",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Benchmark endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033f532-b6dc-4213-93c3-6d8ac75b5da4",
   "metadata": {},
   "source": [
    "Deploy the same model to a ml.g5.xlarge endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1573331-0727-4760-a10e-23deeabdf56c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "hub = {\"HF_MODEL_ID\": MODEL_ID, \"HF_TASK\": \"fill-mask\"}\n",
    "\n",
    "ecr_image = f\"763104351884.dkr.ecr.{sagemaker_session.boto_region_name}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "g5_model = HuggingFaceModel(env=hub, role=sagemaker_execution_role, image_uri=ecr_image)\n",
    "\n",
    "g5_predictor = g5_model.deploy(instance_type=\"ml.g5.xlarge\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890678c-f000-410a-9b4c-2baeaa5fe4e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = \"QVQLVESGGGVVQ<mask>PGRSLTLSCAASGFTFSSYGLHWVRQAPGKGLE\"\n",
    "g5_predictor.predict({\"inputs\": example})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41247a09-b9aa-440e-bf93-5956fdb09b72",
   "metadata": {},
   "source": [
    "Define some benchmarking helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b4334-7dcb-49a9-82ef-6d64c07070bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "\n",
    "def inference_latency(model, inputs):\n",
    "    \"\"\"\n",
    "    inference_latency is a simple method to return the latency of a model inference.\n",
    "\n",
    "        Parameters:\n",
    "            model: torch model onbject loaded using torch.jit.load\n",
    "            inputs: model() args\n",
    "\n",
    "        Returns:\n",
    "            latency in seconds\n",
    "    \"\"\"\n",
    "    error = False\n",
    "    start = time.time()\n",
    "    try:\n",
    "        results = model(inputs)\n",
    "    except:\n",
    "        error = True\n",
    "        results = []\n",
    "    return {\"latency\": time.time() - start, \"error\": error, \"result\": results}\n",
    "\n",
    "\n",
    "uniref = (\n",
    "    load_dataset(\"bloyal/small-uniref30\", split=\"test\")\n",
    "    .remove_columns([\"id\", \"num\"])\n",
    "    .shuffle()[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "def random_sequence(max_length=128):\n",
    "    seq = random.choice(uniref)\n",
    "    seq = list(seq[:max_length])\n",
    "    seq[random.randint(0, len(seq) - 1)] = \"<mask>\"\n",
    "    seq = \"\".join(seq)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def benchmark(predictor, number_of_clients=2, number_of_runs=10000):\n",
    "    # Defining Auxiliary variable\n",
    "\n",
    "    t = tqdm(range(number_of_runs), position=0, leave=True)\n",
    "\n",
    "    # Starting parallel clients\n",
    "    cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "    results = Parallel(n_jobs=number_of_clients, prefer=\"threads\")(\n",
    "        delayed(inference_latency)(predictor.predict, {\"inputs\": random_sequence()})\n",
    "        for mod in t\n",
    "    )\n",
    "    avg_throughput = t.total / t.format_dict[\"elapsed\"]\n",
    "\n",
    "    cw_end = datetime.datetime.utcnow()\n",
    "\n",
    "    # Computing metrics and print\n",
    "    latencies = [res[\"latency\"] for res in results]\n",
    "    errors = [res[\"error\"] for res in results]\n",
    "    error_p = sum(errors) / len(errors) * 100\n",
    "    p50 = np.quantile(latencies[-10000:], 0.50) * 1000\n",
    "    p90 = np.quantile(latencies[-10000:], 0.95) * 1000\n",
    "    p95 = np.quantile(latencies[-10000:], 0.99) * 1000\n",
    "\n",
    "    print(f\"Avg Throughput: :{avg_throughput:.1f}\\n\")\n",
    "    print(f\"50th Percentile Latency:{p50:.1f} ms\")\n",
    "    print(f\"90th Percentile Latency:{p90:.1f} ms\")\n",
    "    print(f\"95th Percentile Latency:{p95:.1f} ms\\n\")\n",
    "    print(f\"Errors percentage: {error_p:.1f} %\\n\")\n",
    "\n",
    "    # Querying CloudWatch\n",
    "    print(\"Getting Cloudwatch:\")\n",
    "    cloudwatch = boto3.client(\"cloudwatch\")\n",
    "    statistics = [\"SampleCount\", \"Average\", \"Minimum\", \"Maximum\"]\n",
    "    extended = [\"p50\", \"p90\", \"p95\", \"p100\"]\n",
    "\n",
    "    # Give 5 minute buffer to end\n",
    "    cw_end += datetime.timedelta(minutes=5)\n",
    "\n",
    "    # Period must be 1, 5, 10, 30, or multiple of 60\n",
    "    # Calculate closest multiple of 60 to the total elapsed time\n",
    "    factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "    period = factor * 60\n",
    "    print(\"Time elapsed: {} seconds\".format((cw_end - cw_start).total_seconds()))\n",
    "    print(\"Using period of {} seconds\\n\".format(period))\n",
    "\n",
    "    cloudwatch_ready = False\n",
    "    # Keep polling CloudWatch metrics until datapoints are available\n",
    "    while not cloudwatch_ready:\n",
    "        time.sleep(30)\n",
    "        print(\"Waiting 30 seconds ...\")\n",
    "        # Must use default units of microseconds\n",
    "        model_latency_metrics = cloudwatch.get_metric_statistics(\n",
    "            MetricName=\"ModelLatency\",\n",
    "            Dimensions=[\n",
    "                {\"Name\": \"EndpointName\", \"Value\": predictor.endpoint_name},\n",
    "                {\"Name\": \"VariantName\", \"Value\": \"AllTraffic\"},\n",
    "            ],\n",
    "            Namespace=\"AWS/SageMaker\",\n",
    "            StartTime=cw_start,\n",
    "            EndTime=cw_end,\n",
    "            Period=period,\n",
    "            Statistics=statistics,\n",
    "            ExtendedStatistics=extended,\n",
    "        )\n",
    "\n",
    "        if len(model_latency_metrics[\"Datapoints\"]) > 0:\n",
    "            print(\n",
    "                \"{} latency datapoints ready\".format(\n",
    "                    model_latency_metrics[\"Datapoints\"][0][\"SampleCount\"]\n",
    "                )\n",
    "            )\n",
    "            side_avg = model_latency_metrics[\"Datapoints\"][0][\"Average\"] / 1000\n",
    "            side_p50 = (\n",
    "                model_latency_metrics[\"Datapoints\"][0][\"ExtendedStatistics\"][\"p50\"]\n",
    "                / 1000\n",
    "            )\n",
    "            side_p90 = (\n",
    "                model_latency_metrics[\"Datapoints\"][0][\"ExtendedStatistics\"][\"p90\"]\n",
    "                / 1000\n",
    "            )\n",
    "            side_p95 = (\n",
    "                model_latency_metrics[\"Datapoints\"][0][\"ExtendedStatistics\"][\"p95\"]\n",
    "                / 1000\n",
    "            )\n",
    "            side_p100 = (\n",
    "                model_latency_metrics[\"Datapoints\"][0][\"ExtendedStatistics\"][\"p100\"]\n",
    "                / 1000\n",
    "            )\n",
    "\n",
    "            print(f\"50th Percentile Latency:{side_p50:.1f} ms\")\n",
    "            print(f\"90th Percentile Latency:{side_p90:.1f} ms\")\n",
    "            print(f\"95th Percentile Latency:{side_p95:.1f} ms\\n\")\n",
    "\n",
    "            cloudwatch_ready = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e715a-ff1b-490a-ad26-fa82f57a2c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark(g5_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3d73b-284f-4863-af27-0e4c6b7076bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark(inf2_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb823de-6bd6-4de6-8525-9e99da585794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inf2_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107bb09b-23a4-4837-8288-87803fddf8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    inf2_predictor.delete_endpoint()\n",
    "    g5_predictor.delete_endpoint()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21c160-ad42-4e97-9c34-8be5aed6685a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.c5.4xlarge",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
