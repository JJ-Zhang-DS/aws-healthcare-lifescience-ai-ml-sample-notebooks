{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. PPI Data Preprocessing\n",
    "\n",
    "In this notebook, we download data from BioGrid and UniProt and add it to a Neptune graph database.\n",
    "\n",
    "This notebook should be uploaded and run on a SageMaker Notebook instance associated with an Amazon Neptune cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U pandas numpy h5py graph-notebook transformers==4.37.2 accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear graph database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%db_reset --generate-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%db_reset --token <REPLACE WITH TOKEN RETURNED BY GENERATE-TOKEN CALL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that no nodes exist after reset job has finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_URI=\"s3://<REPLACE WITH YOUR S3 URI>\"\n",
    "# remove trailing slashes\n",
    "S3_URI = S3_URI[:-1] if S3_URI.endswith('/') else S3_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Get BioGrid Data (Edge Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def download(url: str, filename: str) -> str:\n",
    "    print(f\"Downloading {url} to {filename}\")\n",
    "    output_dir = os.path.dirname(filename)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total = int(r.headers.get(\"content-length\", 0))\n",
    "\n",
    "            tqdm_params = {\n",
    "                \"desc\": url,\n",
    "                \"total\": total,\n",
    "                \"miniters\": 1,\n",
    "                \"unit\": \"B\",\n",
    "                \"unit_scale\": True,\n",
    "                \"unit_divisor\": 1024,\n",
    "            }\n",
    "            with tqdm.tqdm(**tqdm_params) as pb:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    pb.update(len(chunk))\n",
    "                    f.write(chunk)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "BIOGRID_DATA_URI = \"https://downloads.thebiogrid.org/Download/BioGRID/Latest-Release/BIOGRID-MV-Physical-LATEST.tab3.zip\"\n",
    "\n",
    "download(BIOGRID_DATA_URI, os.path.join(DATA_DIR, \"biogrid_mv.zip\"))\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(DATA_DIR, \"biogrid_mv.zip\"), \"r\") as zip_ref:\n",
    "    zip_ref.extractall(DATA_DIR)\n",
    "os.remove(os.path.join(DATA_DIR, \"biogrid_mv.zip\"))\n",
    "BIOGRID_FILE = os.path.join(\n",
    "    DATA_DIR,\n",
    "    [filename for filename in os.listdir(\"data\") if filename.startswith(\"BIOGRID\")][0],\n",
    ")\n",
    "\n",
    "bg = pd.read_csv(\n",
    "    BIOGRID_FILE,\n",
    "    sep=\"\\t\",\n",
    "    usecols=[\n",
    "        \"#BioGRID Interaction ID\",\n",
    "        \"BioGRID ID Interactor A\",\n",
    "        \"BioGRID ID Interactor B\",\n",
    "        \"Official Symbol Interactor A\",\n",
    "        \"Organism ID Interactor A\",\n",
    "        \"Official Symbol Interactor B\",\n",
    "        \"Organism ID Interactor B\",\n",
    "        \"Throughput\",\n",
    "        \"Experimental System\",\n",
    "        \"SWISS-PROT Accessions Interactor A\",\n",
    "        \"SWISS-PROT Accessions Interactor B\",\n",
    "    ],\n",
    ").rename(columns={\"#BioGRID Interaction ID\": \"BioGRID Interaction ID\"})\n",
    "print(f\"All Biogrid MV records: {bg.shape}\")\n",
    "\n",
    "# Remove records with missing SWISS-PROT IDs\n",
    "bg = bg[bg[\"SWISS-PROT Accessions Interactor A\"] != \"-\"]\n",
    "bg = bg[bg[\"SWISS-PROT Accessions Interactor B\"] != \"-\"]\n",
    "print(f\"Records with two SWISS-PROT IDs: {bg.shape}\")\n",
    "\n",
    "# For cases where there are multiple SWISS-PROT IDs, take the first one\n",
    "bg[\"SWISS-PROT Accessions Interactor A\"] = bg[\n",
    "    \"SWISS-PROT Accessions Interactor A\"\n",
    "].str.split(pat=\"|\", expand=True)[0]\n",
    "bg[\"SWISS-PROT Accessions Interactor B\"] = bg[\n",
    "    \"SWISS-PROT Accessions Interactor B\"\n",
    "].str.split(pat=\"|\", expand=True)[0]\n",
    "\n",
    "# Remove records where the protein interacts with itself\n",
    "bg = bg[bg[\"BioGRID ID Interactor A\"] != bg[\"BioGRID ID Interactor B\"]]\n",
    "print(f\"Records with two different proteins: {bg.shape}\")\n",
    "\n",
    "# Remove duplicate entries\n",
    "bg = bg.drop_duplicates()\n",
    "print(f\"Unique rows: {bg.shape}\")\n",
    "\n",
    "\n",
    "bg = bg.sort_values(\n",
    "    by=[\n",
    "        \"Official Symbol Interactor A\",\n",
    "        \"Official Symbol Interactor B\",\n",
    "        \"Throughput\",\n",
    "        \"Experimental System\",\n",
    "    ]\n",
    ")\n",
    "os.remove(BIOGRID_FILE)\n",
    "\n",
    "bg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Get UniProtKB Data (Vertex Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Query UniProt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of unique UniProtKB IDs for both interactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "uniprot_ids = list(\n",
    "    set(\n",
    "        np.concatenate(\n",
    "            [\n",
    "                bg[\"SWISS-PROT Accessions Interactor A\"],\n",
    "                bg[\"SWISS-PROT Accessions Interactor B\"],\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "if None in uniprot_ids:\n",
    "    uniprot_ids.remove(None)\n",
    "uniprot_ids.sort()\n",
    "print(len(uniprot_ids))\n",
    "print(uniprot_ids[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data using UniProt API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uniprot\n",
    "\n",
    "job_id = uniprot.submit_id_mapping(\n",
    "    from_db=\"UniProtKB_AC-ID\", to_db=\"UniProtKB\", ids=uniprot_ids\n",
    ")\n",
    "if uniprot.check_id_mapping_results_ready(job_id):\n",
    "    link = uniprot.get_id_mapping_results_link(job_id)\n",
    "    results = uniprot.get_id_mapping_results_search(link, sequence_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Extract amino acid sequences and other UniProt metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"PrimaryAccession\": result.get(\"to\").get(\"primaryAccession\"),\n",
    "            \"Description\": result.get(\"to\")\n",
    "            .get(\"proteinDescription\")\n",
    "            .get(\"recommendedName\")\n",
    "            .get(\"fullName\")\n",
    "            .get(\"value\"),\n",
    "            \"Organism\": result.get(\"to\").get(\"organism\").get(\"scientificName\"),\n",
    "            \"TaxonId\": result.get(\"to\").get(\"organism\").get(\"taxonId\"),\n",
    "            \"Sequence\": result.get(\"to\").get(\"sequence\").get(\"value\"),\n",
    "            \"Length\": result.get(\"to\").get(\"sequence\").get(\"length\"),\n",
    "            \"MolWeight\": result.get(\"to\").get(\"sequence\").get(\"molWeight\"),\n",
    "            \"Families\": \";\".join(\n",
    "                [\n",
    "                    entry[0][\"value\"]\n",
    "                    for entry in [\n",
    "                        result.get(\"properties\")\n",
    "                        for result in result.get(\"to\").get(\"uniProtKBCrossReferences\")\n",
    "                        if result.get(\"database\") == \"InterPro\"\n",
    "                    ]\n",
    "                ]\n",
    "            ),\n",
    "            \"Keywords\": \";\".join(\n",
    "                [\n",
    "                    f\"{result.get('category')}:{result.get('name')}\"\n",
    "                    for result in result.get(\"to\").get(\"keywords\")\n",
    "                    if result.get(\"category\")\n",
    "                    in [\n",
    "                        \"Cellular component\",\n",
    "                        \"Domain\",\n",
    "                        \"Molecular Function\",\n",
    "                        \"PTM\",\n",
    "                    ]\n",
    "                ]\n",
    "            ),\n",
    "        }\n",
    "        for result in results.get(\"results\")\n",
    "        if result.get(\"to\").get(\"entryType\") != \"Inactive\"\n",
    "    ]\n",
    ").drop_duplicates()\n",
    "print(f\"UniProt records: {seqs.shape}\")\n",
    "seqs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Add Prot-T5 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Download SWISS-PROT Prot-T5 embeddings from UniProt\n",
    "NOTE: This file is around 1.3 GB on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download(\n",
    "    \"https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/embeddings/uniprot_sprot/per-protein.h5\",\n",
    "    \"data/per-protein.h5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Join embeddings to sequence object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "with h5py.File(\"data/per-protein.h5\", \"r\") as f:\n",
    "\n",
    "    def _create_embedding(id, length=1024, dtype=\"float16\"):\n",
    "        arr = np.zeros((length,), dtype=dtype)\n",
    "        try:\n",
    "            dataset = f[id]\n",
    "            dataset.read_direct(arr)\n",
    "            return {\n",
    "                \"PrimaryAccession\": id,\n",
    "                \"prot_t5_embeddings\": \",\".join(map(str, arr.tolist())),\n",
    "            }\n",
    "        except:\n",
    "            next\n",
    "\n",
    "    prot_t5_embeddings = pd.json_normalize(\n",
    "        seqs[\"PrimaryAccession\"].map(_create_embedding)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prot_t5_embeddings.to_csv(\"data/prot_t5_embeddings.csv\", index=False)\n",
    "display(prot_t5_embeddings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you've already calculated the embeddings and just\n",
    "# want to load them from a file.\n",
    "\n",
    "# prott5 = pd.read_csv(\"data/rot_t5_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs = pd.merge(seqs, prot_t5_embeddings, how='inner', on='PrimaryAccession')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add ESM-2 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_embeddings(\n",
    "    text, model_name=\"facebook/esm2_t36_3B_UR50D\", batch_size=24\n",
    "):\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name, device_map=\"auto\", quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    tmp = []\n",
    "    total_batches = len(text) // batch_size\n",
    "    for n, batch in enumerate(\n",
    "        [text[i : i + batch_size] for i in range(0, len(text), batch_size)]\n",
    "    ):\n",
    "        print(f\"Batch {n+1} of {total_batches}\")\n",
    "        inputs = tokenizer(\n",
    "            batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            predictions = model(**inputs)\n",
    "        # Return mean embeddings after removing <cls> and <eos> tokens and converting to numpy.\n",
    "        tmp.append(predictions.last_hidden_state[:, 1:-1, :].numpy().mean(axis=1))\n",
    "    output = np.vstack(tmp)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "esm_2_embeddings = generate_embeddings(\n",
    "    list(seqs[\"Sequence\"]), \n",
    "    batch_size=24, \n",
    "    model_name=\"facebook/esm2_t36_3B_UR50D\"\n",
    ")\n",
    "esm_2_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esm_2_embeddings = list(\n",
    "    map(lambda arr: \",\".join(map(str, arr.tolist())), esm_2_embeddings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esm = pd.DataFrame(\n",
    "    {\"PrimaryAccession\": seqs[\"PrimaryAccession\"], \"esm2\": esm_2_embeddings}\n",
    ")\n",
    "esm.to_csv(\"data/esm_2_embeddings_3B.csv\", index=False)\n",
    "display(esm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Join embeddings to sequence object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment this if you've already calculated the embeddings and just\n",
    "# want to load them from a file.\n",
    "\n",
    "# esm = pd.read_csv(\"data/esm_2_embeddings_3B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs = pd.merge(seqs, esm, how=\"inner\", on=\"PrimaryAccession\").rename(\n",
    "    columns={\"esm2\": \"esm_2_embeddings\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Review data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Node Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(seqs.shape)\n",
    "display(seqs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Edge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(bg.shape)\n",
    "display(bg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7.  Create Neptune Bulk Loader input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bulk_load_dir = os.path.join(DATA_DIR, \"bulk_loader\")\n",
    "if not os.path.exists(bulk_load_dir):\n",
    "    os.makedirs(bulk_load_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Create Vertex Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Protein vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "protein_vertices = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"~id\": seqs[\"PrimaryAccession\"],\n",
    "            \"~label\": \"protein\",\n",
    "            \"description:String\": seqs[\"Description\"],\n",
    "            \"sequence:String\": seqs[\"Sequence\"],\n",
    "            \"length:Int\": seqs[\"Length\"],\n",
    "            \"molWeight:Int\": seqs[\"MolWeight\"],\n",
    "            \"keywords:String[]\": seqs[\"Keywords\"],\n",
    "            \"protT5:String\": seqs[\"prot_t5_embeddings\"],\n",
    "            \"esm2:String\": seqs[\"esm_2_embeddings\"],\n",
    "        }\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=[\"~id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "protein_vertices.to_csv(\n",
    "    os.path.join(bulk_load_dir, \"protein_vertices.csv\"), index=False\n",
    ")\n",
    "display(protein_vertices.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organism vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "organism_vertices = (\n",
    "    pd.DataFrame(\n",
    "        {\"~id\": seqs[\"TaxonId\"], \"~label\": \"organism\", \"name:String\": seqs[\"Organism\"]}\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=[\"~id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "organism_vertices.to_csv(\n",
    "    os.path.join(bulk_load_dir, \"organism_vertices.csv\"), index=False\n",
    ")\n",
    "\n",
    "display(organism_vertices.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Family vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "protein_family = seqs[[\"PrimaryAccession\", \"Families\"]]\n",
    "protein_family[\"Families\"] = protein_family[\"Families\"].map(lambda x: x.split(\";\"))\n",
    "protein_family = protein_family.explode(\"Families\")[\n",
    "    [\"PrimaryAccession\", \"Families\"]\n",
    "].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "family_vertices = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"~id\": protein_family[\"Families\"],\n",
    "            \"~label\": \"family\",\n",
    "            \"name:String\": protein_family[\"Families\"],\n",
    "        }\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=[\"~id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "family_vertices = family_vertices[family_vertices[\"~id\"] != \"\"]\n",
    "\n",
    "family_vertices.to_csv(os.path.join(bulk_load_dir, \"family_vertices.csv\"), index=False)\n",
    "\n",
    "display(family_vertices.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Create edge files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protein-organism edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "protein_organism_edges = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"~id\": seqs[\"PrimaryAccession\"]\n",
    "            + \"-found_in-\"\n",
    "            + seqs[\"TaxonId\"].astype(str),\n",
    "            \"~from\": seqs[\"PrimaryAccession\"],\n",
    "            \"~to\": seqs[\"TaxonId\"],\n",
    "            \"~label\": \"found_in\",\n",
    "        }\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=[\"~id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "protein_organism_edges.to_csv(\n",
    "    os.path.join(bulk_load_dir, \"protein_organism_edges.csv\"), index=False\n",
    ")\n",
    "\n",
    "display(protein_organism_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protein-Family edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "protein_family_edges = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"~id\": protein_family[\"PrimaryAccession\"]\n",
    "            + \"-member_of-\"\n",
    "            + protein_family[\"Families\"].astype(str),\n",
    "            \"~from\": protein_family[\"PrimaryAccession\"],\n",
    "            \"~to\": protein_family[\"Families\"],\n",
    "            \"~label\": \"member_of\",\n",
    "        }\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=[\"~id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "protein_family_edges.to_csv(\n",
    "    os.path.join(bulk_load_dir, \"protein_family_edges.csv\"), index=False\n",
    ")\n",
    "\n",
    "display(protein_family_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protein-Protein edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "protein_protein_edges = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"~id\": bg[\"SWISS-PROT Accessions Interactor A\"]\n",
    "            + \"-interacts_with-\"\n",
    "            + bg[\"SWISS-PROT Accessions Interactor B\"],\n",
    "            \"~from\": bg[\"SWISS-PROT Accessions Interactor A\"],\n",
    "            \"~to\": bg[\"SWISS-PROT Accessions Interactor B\"],\n",
    "            \"~label\": \"interacts_with\",\n",
    "            \"experimentalSystem:String\": bg[\"Experimental System\"],\n",
    "            \"throughput:String\": bg[\"Throughput\"],\n",
    "        }\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=[\"~id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "protein_protein_edges.to_csv(\n",
    "    os.path.join(bulk_load_dir, \"protein_protein_edges.csv\"), index=False\n",
    ")\n",
    "display(protein_protein_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import s3\n",
    "\n",
    "uploader = s3.S3Uploader()\n",
    "uploader.upload(\"data/bulk_loader/\", S3_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load data into Neptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Verify Neptune Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext graph_notebook.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%graph_notebook_version\n",
    "%graph_notebook_config\n",
    "%status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Start bulk loading job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell and select \"Submit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load -s $S3_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to verify that data loaded successfully into Neptune. Note that the loader may produce errors for a small number of records - this is fine.\n",
    "\n",
    "The final result should be approximately:\n",
    "\n",
    "Protein: 23571\n",
    "Taxon: 53\n",
    "Family: 16447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().groupCount().by(label).unfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
