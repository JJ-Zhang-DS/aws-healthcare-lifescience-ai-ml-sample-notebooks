{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Use SageMaker Training Jobs to Accelerate Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Use SageMaker processing and training jobs to optimize cost and performance\n",
    "- Compare model performance using SageMaker Experiments\n",
    "- Deploy a model as an endpoint for real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Notes:\n",
    "This notebook was created and tested on an `ml.t3.medium (2 vCPU + 4 GiB)` notebook instance running the `Python 3 (Data Science 3.0)` kernel in SageMaker Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Background](#1.-Background)  \n",
    "    1.1. [SageMaker Jobs](#1.1.-SageMaker-Jobs)  \n",
    "    1.2. [SageMaker Experiments](#1.2.-SageMaker-Experiments)  \n",
    "2. [Preparation](#2.-Preparation)  \n",
    "    2.1. [Import Python libraries](#2.1.-Import-Python-Libraries)  \n",
    "    2.2. [Create Some Necessary Clients](#2.2.-Create-some-necessary-clients)  \n",
    "    2.3. [Create an Experiment](#2.3.-Create-an-experiment)  \n",
    "    2.4. [Specify S3 Bucket and Prefix](#2.4.-Specify-S3-bucket-and-prefix)  \n",
    "    2.5. [Define Local Working Directories](#2.5.-Define-local-working-directories)  \n",
    "3. [Data Preparation with Amazon SageMaker Processing](#3.-Data-Preparation-with-Amazon-SageMaker-Processing)  \n",
    "    3.1. [Submit SageMaker Processing Job](#3.1.-Submit-SageMaker-Processing-Job)  \n",
    "    3.2. [Download Processed Data from S3](#3.2.-Download-Processed-Data-from-S3)  \n",
    "4. [Model Training](#4.-Model-Training)  \n",
    "    4.1. [Train Model Using a SKLearn Random Forest Algorithm](#4.1.-Train-Model-Using-a-SKLearn-Random-Forest-Algorithm)  \n",
    "    4.2. [Train Model using a Keras MLP](#4.2.-Train-Model-using-a-Keras-MLP)  \n",
    "    4.3. [Train Model Using the XGBoost Algorithm](#4.3.-Train-Model-Using-the-XGBoost-Algorithm)  \n",
    "5. [Model Evaluation](#5.-Model-Evaluation)  \n",
    "    5.1. [Download and Run the Trained XGBoost Model](#5.1.-Download-and-Run-the-Trained-XGBoost-Model)  \n",
    "    5.2. [Compare Model Results Using SageMaker Experiments](#5.2.-Compare-Model-Results-Using-SageMaker-Experiments)  \n",
    "6. [Deploy](#6-Deploy)  \n",
    "    6.1. [Deploy Model as a SageMaker Endpoint](#6.1.-Deploy-Model-as-a-SageMaker-Endpoint)  \n",
    "    6.2. [Test Endpoint](#6.2.-Test-Endpoint)  \n",
    "    6.3. [Clean Up](#6.3.-Clean-Up)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Background\n",
    "In notebook 1 of this series, we demonstrated using RNAseq data to predict HER2 status using the compute resources on the notebook server. However, using notebook server resources to process large amounts of data or train complex models is generally not a good idea. It's possible to scale up your notebook server, but any time you spend on non-compute intensive tasks (i.e. most of your time) will be wasted. A better idea is to run your notebook on a small server and submit compute-intensive tasks to independent jobs. SageMaker provides managed services for running data processing, model training, and hyperparameter tuning jobs. In this notebook, we'll demonstrate how to leverage these services to optimize the performance and cost of our tasks.\n",
    "\n",
    "Specifically, we'll demonstrate two best practices: **Jobs** and **Experiments**.\n",
    "\n",
    "These best practices play a key role in the **Prepare Data** and **Model Development** phases of the Machine Learning Lifecycle. For more information, please refer to the [Machine Learning Best Practices in Healthcare and Life Sciences Whitepaper](https://d1.awsstatic.com/whitepapers/ML-best-practices-health-science.pdf?did=wp_card&trk=wp_card).\n",
    "\n",
    "![Machine Learning Life Cycle - Part 1](img/MLLC1.png \"ML Life Cycle - Part 1\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. SageMaker Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html) processing, training, and hyperparameter optimization (HPO) jobs allow data scientists to submit compute-heavy processes to external services. This keeps costs optimized and ensures that these tasks run in reproducible environments. It also improves data scientist productivity by allowing these jobs to run in \"the background\" and provides resiliancy if something happens to your notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/jobs.png \"Jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. SageMaker Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/experiments.png \"Experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SageMaker Experiments](https://aws.amazon.com/blogs/aws/amazon-sagemaker-experiments-organize-track-and-compare-your-machine-learning-trainings) make it as easy as possible to track data preparation and analysis steps. Organizing your ML project into experiments helps you manage large numbers of trials and alternative algorithms. Experiments also ensure that any artifacts your generate for production use can be traced back to their source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Preparation\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The Python libraries that we'll use throughout the analysis\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --disable-pip-version-check -U -q sagemaker sagemaker-experiments xgboost seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sagemaker\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from sagemaker.processing import ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.search_expression import Filter, Operator, SearchExpression\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from time import strftime, sleep\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Create Some Necessary Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name\n",
    "sagemaker_session = sagemaker.session.Session(boto_session)\n",
    "sagemaker_execution_role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "s3_boto_client = boto_session.client(\"s3\")\n",
    "account_id = boto_session.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "print(f\"Assumed SageMaker role is {sagemaker_execution_role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Create an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new SageMaker experiment specific to our scientific goal, in this case to predict HER2 status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "brca_her2_experiment = Experiment.create(\n",
    "    description=\"Predict HER2 status using TCGA RNAseq data.\",\n",
    "    experiment_name=f\"BRCA-HER2-{create_date}\",\n",
    "    sagemaker_boto_client=sagemaker_boto_client,\n",
    "    tags=[{\"Key\": \"Creator\", \"Value\": \"arosalez\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Specify S3 Bucket and Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 path is sagemaker-us-west-2-167428594774/brca-her2-classifier\n"
     ]
    }
   ],
   "source": [
    "S3_BUCKET = sagemaker_session.default_bucket()\n",
    "S3_PREFIX = \"brca-her2-classifier\"\n",
    "S3_PATH = sagemaker.s3.s3_path_join(S3_BUCKET, S3_PREFIX)\n",
    "print(f\"S3 path is {S3_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Define Local Working Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory is /root/aws-healthcare-lifescience-ai-ml-sample-notebooks/workshops/RNAseq_Tertiary_Analysis\n",
      "Data directory is /root/aws-healthcare-lifescience-ai-ml-sample-notebooks/workshops/RNAseq_Tertiary_Analysis/data\n"
     ]
    }
   ],
   "source": [
    "WORKING_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "print(f\"Working directory is {WORKING_DIR}\")\n",
    "print(f\"Data directory is {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preparation  with Amazon SageMaker Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Processing allows you to run steps for data pre- or post-processing, feature engineering, data validation, or model evaluation workloads on Amazon SageMaker. Processing jobs accept data from Amazon S3 as input and store data into Amazon S3 as output.\n",
    "\n",
    "![processing](https://sagemaker.readthedocs.io/en/stable/_images/amazon_sagemaker_processing_image1.png)\n",
    "\n",
    "Here, we'll import the dataset and transform it with SageMaker Processing, which can be used to process terabytes of data in a SageMaker-managed cluster separate from the instance running your notebook server. In a typical SageMaker workflow, notebooks are only used for prototyping and can be run on relatively inexpensive and less powerful instances, while processing, training and model hosting tasks are run on separate, more powerful SageMaker-managed instances.  SageMaker Processing includes off-the-shelf support for Scikit-learn, as well as a Bring Your Own Container option, so it can be used with many different data transformation technologies and tasks.    \n",
    "\n",
    "To use SageMaker Processing, simply supply a Python data preprocessing script as shown below.  For this example, we're using a SageMaker prebuilt Scikit-learn container, which includes many common functions for processing data.  There are few limitations on what kinds of code and operations you can run, and only a minimal contract:  input and output data must be placed in specified directories.  If this is done, SageMaker Processing automatically loads the input data from S3 and uploads transformed data back to S3 when the job is complete.\n",
    "\n",
    "For this example, we'll download the raw data directly from Xenahubs as part of the processing script, so we do not need to specify an input bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Submit SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take about 5 minutes to complete. Notice that this code block references `scripts/processing/processing.py`. The processing job will run this script on a different compute instance, in this case a ml.m5.xlarge. This allows us to use a small instance for our notebook server, while still taking advantage of a more powerful instance for the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker:Creating processing-job with name data-processing-job-2022-12-15-22-32-52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  data-processing-job-2022-12-15-22-32-52\n",
      "Inputs:  [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-167428594774/data-processing-job-2022-12-15-22-32-52/input/code/processing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-167428594774/brca-her2-classifier/data/train/', 'LocalPath': '/opt/ml/processing/output/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'validation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-167428594774/brca-her2-classifier/data/val/', 'LocalPath': '/opt/ml/processing/output/val', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-167428594774/brca-her2-classifier/data/test/', 'LocalPath': '/opt/ml/processing/output/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".........................\u001b[34mData directory is /opt/ml/processing/input\u001b[0m\n",
      "\u001b[34m2022-12-15 22:37:46 URL:https://tcga-xena-hub.s3.us-east-1.amazonaws.com/download/TCGA.BRCA.sampleMap/HiSeqV2_PANCAN.gz [83389754/83389754] -> \"/opt/ml/processing/input/HiSeqV2_PANCAN.gz\" [1]\u001b[0m\n",
      "\u001b[34m2022-12-15 22:37:50 URL:https://tcga-xena-hub.s3.us-east-1.amazonaws.com/download/TCGA.BRCA.sampleMap/BRCA_clinicalMatrix [1698413/1698413] -> \"/opt/ml/processing/input/BRCA_clinicalMatrix\" [1]\u001b[0m\n",
      "\u001b[34m['HiSeqV2_PANCAN', 'BRCA_clinicalMatrix', 'code']\u001b[0m\n",
      "\u001b[34m/opt/ml/processing/input/HiSeqV2_PANCAN\u001b[0m\n",
      "\u001b[34mThe training data has 974 records and 20001 columns.\u001b[0m\n",
      "\u001b[34mThe validation data has 244 records and 20001 columns.\u001b[0m\n",
      "\u001b[34mThe test data has 122 records and 20001 columns.\u001b[0m\n",
      "\u001b[34mTraining data saved to /opt/ml/processing/output/train/train.csv\u001b[0m\n",
      "\u001b[34mValidation data saved to /opt/ml/processing/output/val/val.csv\u001b[0m\n",
      "\u001b[34mTest data saved to /opt/ml/processing/output/test/test.csv\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HISEQ_URL = \"https://tcga.xenahubs.net/download/TCGA.BRCA.sampleMap/HiSeqV2_PANCAN.gz\"\n",
    "BRCA_CLINICAL_MATRIX_URL = (\n",
    "    \"https://tcga.xenahubs.net/download/TCGA.BRCA.sampleMap/BRCA_clinicalMatrix\"\n",
    ")\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    role=sagemaker_execution_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "processing_run_name = f\"data-processing-job-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "sklearn_processor.run(\n",
    "    job_name=processing_run_name,\n",
    "    code=\"scripts/processing/processing.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",\n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/train/\",\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"validation\",\n",
    "            source=\"/opt/ml/processing/output/val\",\n",
    "            destination=f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/val/\",\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\",\n",
    "            source=\"/opt/ml/processing/output/test\",\n",
    "            destination=f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/test/\",\n",
    "        ),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--brca_clinical_matrix_url\",\n",
    "        BRCA_CLINICAL_MATRIX_URL,\n",
    "        \"--hiseq_url\",\n",
    "        HISEQ_URL,\n",
    "        \"--train_test_split_ratio\",\n",
    "        \"0.2\",\n",
    "        \"--gene_count\",\n",
    "        \"20000\",\n",
    "        \"--create_test_data\",\n",
    "    ],\n",
    "    experiment_config={\n",
    "        \"ExperimentName\": brca_her2_experiment.experiment_name,\n",
    "        \"TrialComponentDisplayName\": processing_run_name,\n",
    "    },\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Download Processed Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.download_data(\n",
    "    f\"{DATA_DIR}/output/train\",\n",
    "    bucket=S3_BUCKET,\n",
    "    key_prefix=f\"{S3_PREFIX}/data/train/train.csv\",\n",
    ")\n",
    "sagemaker_session.download_data(\n",
    "    f\"{DATA_DIR}/output/val\",\n",
    "    bucket=S3_BUCKET,\n",
    "    key_prefix=f\"{S3_PREFIX}/data/val/val.csv\",\n",
    ")\n",
    "sagemaker_session.download_data(\n",
    "    f\"{DATA_DIR}/output/test\",\n",
    "    bucket=S3_BUCKET,\n",
    "    key_prefix=f\"{S3_PREFIX}/data/test/test.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our training data is set up, we can train some models. To highlight the benefits of experiment tracking, we're going to train models using three different frameworks:\n",
    "- The random forest model from Scikit Learm\n",
    "- A multi-layer perceptron (MLP) neural network in Keras\n",
    "- The open-source XGBoost algorithm\n",
    "\n",
    "Since we're using SageMaker jobs to run our training, we don't need to install any additional libraries or spin up expensive compute resources on our notebook server. The jobs use their own dependencies and we're only charged for the time they run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define some variables that all three training jobs will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"text/csv\"\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/train/train.csv\", content_type=content_type\n",
    ")\n",
    "\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "    f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/val/val.csv\", content_type=content_type\n",
    ")\n",
    "\n",
    "model_output_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Train Model Using a SKLearn Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again we're passing a script (`scripts/rf_train/rf_train.py`) to run during the training job. Notice that we've also included a `requirements.txt` file in the training script directory to install additional dependencies in the training container. This is a great way to install an extra package or two without creating your own container image from scratch! Setting `wait=False` allows us to continue running the notebook while the training job runs in \"the background\" (on a different machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: RF-Training-Job-2022-12-15-22-39-21\n"
     ]
    }
   ],
   "source": [
    "rf_trial = Trial.create(\n",
    "    experiment_name=brca_her2_experiment.experiment_name,\n",
    "    sagemaker_boto_client=sagemaker_boto_client,\n",
    "    trial_name=f\"RF-Trial-{strftime('%Y-%m-%d-%H-%M-%S')}\",\n",
    ")\n",
    "\n",
    "rf_job_name = f\"RF-Training-Job-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "rf_estimator = SKLearn(\n",
    "    base_job_name=rf_job_name,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    entry_point=\"rf_train.py\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    hyperparameters={\n",
    "        \"n-estimators\": 100,\n",
    "        \"min-samples-leaf\": 3,\n",
    "    },\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    output_path=model_output_path,\n",
    "    role=sagemaker_execution_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    source_dir=\"scripts/rf_train\",\n",
    ")\n",
    "\n",
    "rf_estimator.fit(\n",
    "    {\"train\": s3_input_train, \"validation\": s3_input_validation},\n",
    "    experiment_config={\n",
    "        \"TrialComponentDisplayName\": rf_job_name,\n",
    "        \"TrialName\": rf_trial.trial_name,\n",
    "    },\n",
    "    job_name=rf_job_name,\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Train Model using a Keras MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: TF-Training-Job-2022-12-15-22-43-37\n"
     ]
    }
   ],
   "source": [
    "tf_trial = Trial.create(\n",
    "    experiment_name=brca_her2_experiment.experiment_name,\n",
    "    sagemaker_boto_client=sagemaker_boto_client,\n",
    "    trial_name=f\"TF-Trial-{strftime('%Y-%m-%d-%H-%M-%S')}\",\n",
    ")\n",
    "\n",
    "tf_job_name = f\"TF-Training-Job-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    enable_sagemaker_metrics=True,\n",
    "    entry_point=\"tf_train.py\",\n",
    "    framework_version=\"2.2\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"validation:accuracy\", \"Regex\": \"Validation Accuracy: ([0-9.]+)$\"},\n",
    "        {\"Name\": \"validation:precision\", \"Regex\": \"Validation Precision: ([0-9.]+)$\"},\n",
    "        {\"Name\": \"validation:f1\", \"Regex\": \"Validation F1 Score: ([0-9.]+)$\"},\n",
    "    ],\n",
    "    output_path=model_output_path,\n",
    "    py_version=\"py37\",\n",
    "    role=sagemaker_execution_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    source_dir=\"scripts/tf_train\",\n",
    ")\n",
    "\n",
    "tf_estimator.fit(\n",
    "    {\"train\": s3_input_train, \"validation\": s3_input_validation},\n",
    "    experiment_config={\n",
    "        \"TrialComponentDisplayName\": tf_job_name,\n",
    "        \"TrialName\": tf_trial.trial_name,\n",
    "    },\n",
    "    job_name=tf_job_name,\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Train Model Using the XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the XGBoost training script we're about to run (scripts/rf_train/rf_train.py) with the training function we used in Notebook 1. You'll notice that the `xgb.train` call is the same in both! Since we're setting `wait=True` our Jupyter session will wait until this training job is finished before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: XGB-Training-Job-2022-12-15-22-39-23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15 22:39:24 Starting - Starting the training job...\n",
      "2022-12-15 22:39:50 Starting - Preparing the instances for trainingProfilerReport-1671143963: InProgress\n",
      "............\n",
      "2022-12-15 22:41:51 Downloading - Downloading input data\n",
      "2022-12-15 22:41:51 Training - Training image download completed. Training in progress..\u001b[34m[2022-12-15 22:41:48.428 ip-10-0-133-243.us-west-2.compute.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Invoking user training script.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Module xgb_train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Generating setup.cfg\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting sagemaker-experiments==0.1.35\n",
      "  Downloading sagemaker_experiments-0.1.35-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 3.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.16.27 in /miniconda3/lib/python3.7/site-packages (from sagemaker-experiments==0.1.35->-r requirements.txt (line 1)) (1.17.52)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /miniconda3/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments==0.1.35->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.21.0,>=1.20.52 in /miniconda3/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments==0.1.35->-r requirements.txt (line 1)) (1.20.52)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /miniconda3/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments==0.1.35->-r requirements.txt (line 1)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.52->boto3>=1.16.27->sagemaker-experiments==0.1.35->-r requirements.txt (line 1)) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /miniconda3/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.52->boto3>=1.16.27->sagemaker-experiments==0.1.35->-r requirements.txt (line 1)) (1.26.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /miniconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.52->boto3>=1.16.27->sagemaker-experiments==0.1.35->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: xgb-train\n",
      "  Building wheel for xgb-train (setup.py): started\n",
      "  Building wheel for xgb-train (setup.py): finished with status 'done'\n",
      "  Created wheel for xgb-train: filename=xgb_train-1.0.0-py2.py3-none-any.whl size=6242 sha256=d6e72867ee287c57f63ea2cff543e2e2d9290efc90cd078ba3c91946a75d52ef\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-551p3d15/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built xgb-train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xgb-train, sagemaker-experiments\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-experiments-0.1.35 xgb-train-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"booster\": \"gbtree\",\n",
      "        \"eval_metric\": \"error\",\n",
      "        \"max_depth\": 3,\n",
      "        \"min_child_weight\": 5,\n",
      "        \"objective\": \"binary:logistic\",\n",
      "        \"scale_pos_weight\": 9.0,\n",
      "        \"subsample\": 0.9,\n",
      "        \"tree_method\": \"auto\",\n",
      "        \"verbosity\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"XGB-Training-Job-2022-12-15-22-39-23\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-167428594774/XGB-Training-Job-2022-12-15-22-39-23/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"xgb_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"xgb_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"booster\":\"gbtree\",\"eval_metric\":\"error\",\"max_depth\":3,\"min_child_weight\":5,\"objective\":\"binary:logistic\",\"scale_pos_weight\":9.0,\"subsample\":0.9,\"tree_method\":\"auto\",\"verbosity\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=xgb_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=xgb_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-167428594774/XGB-Training-Job-2022-12-15-22-39-23/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"booster\":\"gbtree\",\"eval_metric\":\"error\",\"max_depth\":3,\"min_child_weight\":5,\"objective\":\"binary:logistic\",\"scale_pos_weight\":9.0,\"subsample\":0.9,\"tree_method\":\"auto\",\"verbosity\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"XGB-Training-Job-2022-12-15-22-39-23\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-167428594774/XGB-Training-Job-2022-12-15-22-39-23/source/sourcedir.tar.gz\",\"module_name\":\"xgb_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"xgb_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--booster\",\"gbtree\",\"--eval_metric\",\"error\",\"--max_depth\",\"3\",\"--min_child_weight\",\"5\",\"--objective\",\"binary:logistic\",\"--scale_pos_weight\",\"9.0\",\"--subsample\",\"0.9\",\"--tree_method\",\"auto\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_BOOSTER=gbtree\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_METRIC=error\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=3\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_CHILD_WEIGHT=5\u001b[0m\n",
      "\u001b[34mSM_HP_OBJECTIVE=binary:logistic\u001b[0m\n",
      "\u001b[34mSM_HP_SCALE_POS_WEIGHT=9.0\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_TREE_METHOD=auto\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m xgb_train --booster gbtree --eval_metric error --max_depth 3 --min_child_weight 5 --objective binary:logistic --scale_pos_weight 9.0 --subsample 0.9 --tree_method auto --verbosity 1\u001b[0m\n",
      "\u001b[34mINFO:root:Extracting arguments\u001b[0m\n",
      "\u001b[34mINFO:root:Namespace(alpha=0, booster='gbtree', eta=0.3, eval_metric='error', gamma=0, max_depth=3, min_child_weight=5, model_dir='/opt/ml/model', num_round=25, objective='binary:logistic', predictor='auto', scale_pos_weight=9.0, subsample=0.9, test=None, test_file='test.csv', train='/opt/ml/input/data/train', train_file='train.csv', tree_method='auto', validation='/opt/ml/input/data/validation', validation_file='val.csv', verbosity=1)\u001b[0m\n",
      "\u001b[34mINFO:root:Preparing data\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/xgboost/data.py:96: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\u001b[0m\n",
      "\u001b[34mINFO:root:Training model\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.09138#011validation-error:0.13115\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.03388#011validation-error:0.04918\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.03901#011validation-error:0.04918\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.02156#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.02361#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.01951#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.02156#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.01540#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.01335#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.01232#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.00821#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.00719#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.00719#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.00616#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.00616#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.00411#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.00411#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.00205#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.00205#011validation-error:0.02459\u001b[0m\n",
      "\n",
      "2022-12-15 22:42:22 Uploading - Uploading generated training model\u001b[34m[19]#011train-error:0.00103#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.00103#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.00103#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.00103#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.00103#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.00103#011validation-error:0.02459\u001b[0m\n",
      "\u001b[34mINFO:root:Stored trained model at /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34mINFO:root:Evaluating model\u001b[0m\n",
      "\u001b[34mINFO:root:Validation Accuracy: 0.98\u001b[0m\n",
      "\u001b[34mINFO:root:Validation Precision: 1.00\u001b[0m\n",
      "\u001b[34mINFO:root:Validation F1 Score: 0.84\u001b[0m\n",
      "\n",
      "2022-12-15 22:42:51 Completed - Training job completed\n",
      "Training seconds: 67\n",
      "Billable seconds: 67\n"
     ]
    }
   ],
   "source": [
    "xgb_trial = Trial.create(\n",
    "    experiment_name=brca_her2_experiment.experiment_name,\n",
    "    sagemaker_boto_client=sagemaker_boto_client,\n",
    "    trial_name=f\"XGBoost-Trial-{strftime('%Y-%m-%d-%H-%M-%S')}\",\n",
    ")\n",
    "\n",
    "xgb_job_name = f\"XGB-Training-Job-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "framework_version = \"1.2-1\"\n",
    "py_version = \"py3\"\n",
    "\n",
    "hyper_params_dict = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"eval_metric\": \"error\",\n",
    "    \"scale_pos_weight\": 9.0,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"subsample\": 0.9,\n",
    "    \"verbosity\": 1,\n",
    "    \"tree_method\": \"auto\",\n",
    "}\n",
    "\n",
    "xgb_estimator = XGBoost(\n",
    "    enable_sagemaker_metrics=True,\n",
    "    entry_point=\"xgb_train.py\",\n",
    "    framework_version=framework_version,\n",
    "    hyperparameters=hyper_params_dict,\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", region, framework_version)\n",
    "    + \"-cpu-\"\n",
    "    + py_version,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=model_output_path,\n",
    "    py_version=py_version,\n",
    "    role=sagemaker_execution_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    source_dir=\"scripts/xgb_train\",\n",
    ")\n",
    "\n",
    "xgb_estimator.fit(\n",
    "    {\"train\": s3_input_train, \"validation\": s3_input_validation},\n",
    "    experiment_config={\n",
    "        \"TrialName\": xgb_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": xgb_job_name,\n",
    "    },\n",
    "    job_name=xgb_job_name,\n",
    "    logs=True,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Compare Model Results Using SageMaker Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Experiments saves key information about our models for easy viewing and comparison in the SageMaker Studio UI.\n",
    "\n",
    "To start, click on the Home icon on the Studio sidebar and select `Experiments` from the menu.\n",
    "\n",
    "![alt text](img/exp-1.png \"Studio Resources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view information about your experiment click on the name (should start with \"BRCA-HER2-\" and then select `Open in trial component list`.\n",
    "\n",
    "![alt text](img/exp-2.png \"Experiment List\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Trial Component list has a record for each of the training jobs, plus the processing job. You can click on a trial component name for more information about that job.\n",
    "\n",
    "![alt text](img/exp-3.png \"Trial Components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the performance of our model training jobs by adding an additional metric to the table. To do this, click on the Gear on the Studio right-sidebar and then select `validation:f1` in the **Metrics** section.\n",
    "\n",
    "![alt text](img/metrics.png \"Metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the XGBoost model had the highest f1 score on the validation data. Note that your exact results may vary.\n",
    "\n",
    "![alt text](img/exp-4.png \"Validation Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the same information programmatically by using the `ExperimentAnalytics` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression = {\n",
    "    \"Filters\": [\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Contains\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    experiment_name=brca_her2_experiment.experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.validation:f1.last\",\n",
    "    sort_order=\"Descending\",\n",
    "    metric_names=[\"validation:f1\"],\n",
    "    parameter_names=[\"SageMaker.InstanceType\"],\n",
    ")\n",
    "\n",
    "trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Download and Evaluate the Trained XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Notebook 1, we used a confusion matrix to evaluate the accuracy of our model. Let's download our trained XGBoost model and do the same thing here.\n",
    "\n",
    "First, we download the model artifact from S3 and load it into our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.download_data(\n",
    "    \"models\",\n",
    "    bucket=S3_BUCKET,\n",
    "    key_prefix=f\"{S3_PREFIX}/models/{xgb_job_name}/output/model.tar.gz\",\n",
    ")\n",
    "!tar xvfz models/model.tar.gz -C models\n",
    "\n",
    "loaded_model = pickle.load(open(\"models/xgboost-model\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read in the test data and seperate it into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DATA_DIR}/output/test/test.csv\", \"rb\") as file:\n",
    "    test_np = np.loadtxt(file, delimiter=\",\")\n",
    "test_labels = test_np[:, 0]\n",
    "test_np = test_np[:, 1:]\n",
    "test_dm = xgb.DMatrix(test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate predictions\n",
    "test_predictions = loaded_model.predict(test_dm)\n",
    "accuracy = accuracy_score(test_labels, np.rint(test_predictions))\n",
    "accuracy = accuracy_score(test_labels, np.rint(test_predictions))\n",
    "precision = precision_score(test_labels, np.rint(test_predictions))\n",
    "f1 = f1_score(test_labels, np.rint(test_predictions))\n",
    "\n",
    "p = 0.5\n",
    "cm = confusion_matrix(test_labels, np.array(test_predictions) > p)\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion matrix @{:.2f}\".format(p))\n",
    "plt.ylabel(\"Actual label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.show()\n",
    "\n",
    "if len(set(list(test_labels))) == 2:\n",
    "    print(\"Correctly un-detected (True Negatives): \", cm[0][0])\n",
    "    print(\"Incorrectly detected (False Positives): \", cm[0][1])\n",
    "    print(\"Misses (False Negatives): \", cm[1][0])\n",
    "    print(\"Hits (True Positives): \", cm[1][1])\n",
    "    print(\"Total: \", np.sum(cm[1]))\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll look at options for deploying this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained a model, we can allow other applications to use it for inference by deploying it. SageMaker offers several deployment options, based on your performance, cost, and data needs:\n",
    "\n",
    "![alt text](img/deployment_options.png \"SageMaker Model Deployment Options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Deploy Model as SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-time inference endpoints are deployed to a persistent EC2 instance. This allows them to respond quickly to requests and support a wide range of custom properties. It's a good choice for models with steady usage. Deploying this endpoint will take about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "realtime_endpoint_name = f\"her2-real-time-endpoint-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "xgb_predictor = xgb_estimator.deploy(\n",
    "    endpoint_name=realtime_endpoint_name,\n",
    "    serializer=sagemaker.serializers.CSVSerializer(),  # Helper function to serialize ndarray into buffer\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),  # Helper function to deserialize buffer into ndarray\n",
    "    wait=True,\n",
    "    instance_type=\"ml.t2.medium\",  # Instance type we want to use to host our endpoint.\n",
    "    initial_instance_count=1,  # For this example, we'll only use a single hosting instance.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load a random sample of 10 records from the test data\n",
    "test_df = pd.read_csv(\"data/output/test/test.csv\", header=None).sample(n=25)\n",
    "\n",
    "# Submit the 10 samples to the inference endpoint and compare the actual and predicted values\n",
    "print(\n",
    "    f\"Sending test traffic to the endpoint {xgb_predictor.endpoint_name}. \\nPlease wait...\"\n",
    ")\n",
    "\n",
    "for i, row in test_df.iterrows():\n",
    "    print(\n",
    "        f\"[Actual | predicted] labels for record {i:3} are [{row[0]} | {xgb_predictor.predict([row.iloc[1:]])[0]:.3f}]\"\n",
    "    )\n",
    "    sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint\n",
    "xgb_predictor.delete_endpoint()\n",
    "\n",
    "# Delete all S3 objects\n",
    "bucket = boto_session.resource(\"s3\").Bucket(S3_BUCKET)\n",
    "bucket.objects.filter().delete()\n",
    "\n",
    "os.system(\"rm -rf data models generated training_reports\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "fcd20795596b30c7734a8efd08df92d501ca130112f67abeee93ccff645bf25b"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
