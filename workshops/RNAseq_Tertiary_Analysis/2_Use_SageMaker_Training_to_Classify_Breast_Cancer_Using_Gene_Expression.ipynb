{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Use SageMaker Training Jobs to Accelerate Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Use SageMaker processing and training jobs to optimize cost and performance\n",
    "- Compare model performance using SageMaker Experiments\n",
    "- Deploy a model as an endpoint for real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Notes:\n",
    "This notebook was created and tested on an `ml.t3.medium (2 vCPU + 4 GiB)` notebook instance running the `Python 3.0 (Data Science)` kernel in SageMaker Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Background](#1.-Background)\n",
    "    1. [SageMaker Jobs](#1.1.-SageMaker-Jobs)\n",
    "    1. [SageMaker Experiments](#1.2.-SageMaker-Experiments)\n",
    "1. [Preparation](#2.-Preparation)\n",
    "    1. [Import Python libraries](#2.1.-Import-Python-Libraries)\n",
    "    1. [Create Some Necessary Clients](#2.2.-Create-some-necessary-clients)\n",
    "    1. [Create an Experiment](#2.3.-Create-an-experiment)\n",
    "    1. [Specify S3 Bucket and Prefix](#2.4.-Specify-S3-bucket-and-prefix)\n",
    "    1. [Define Local Working Directories](#2.5.-Define-local-working-directories)\n",
    "1. [Data Preparation with Amazon SageMaker Processing](#3.-Data-Preparation-with-Amazon-SageMaker-Processing)\n",
    "    1. [Submit SageMaker Processing Job](#3.1.-Submit-SageMaker-Processing-Job)\n",
    "    1. [Download Processed Data from S3](#3.2.-Download-Processed-Data-from-S3)\n",
    "1. [Model Training](#4.-Model-Training)\n",
    "    1. [Train Model Using a SKLearn Random Forest Algorithm](#4.1.-Train-Model-Using-a-SKLearn-Random-Forest-Algorithm)\n",
    "    1. [Train Model using a Keras MLP](#4.2.-Train-Model-using-a-Keras-MLP)\n",
    "    1. [Train Model Using the XGBoost Algorithm](#4.3.-Train-Model-Using-the-XGBoost-Algorithm)\n",
    "1. [Model Evaluation](#5.-Model-Evaluation)\n",
    "    1. [Download and Run the Trained XGBoost Model](#5.1.-Download-and-Run-the-Trained-XGBoost-Model)\n",
    "    1. [Compare Model Results Using SageMaker Experiments](#5.2.-Compare-Model-Results-Using-SageMaker-Experiments)\n",
    "1. [Deploy](#6-Deploy)\n",
    "    1. [Deploy Model as a SageMaker Endpoint](#6.1.-Deploy-Model-as-a-SageMaker-Endpoint)\n",
    "    1. [Test Endpoint](#6.2.-Test-Endpoint)\n",
    "    1. [Clean Up](#6.3.-Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Background\n",
    "In notebook 1 of this series, we demonstrated using RNAseq data to predict HER2 status using the compute resources on the notebook server. However, using notebook server resources to process large amounts of data or train complex models is generally not a good idea. It's possible to scale up your notebook server, but any time you spend on non-compute intensive tasks (i.e. most of your time) will be wasted. A better idea is to run your notebook on a small server and submit compute-intensive tasks to independent jobs. SageMaker provides managed services for running data processing, model training, and hyperparameter tuning jobs. In this notebook, we'll demonstrate how to leverage these services to optimize the performance and cost of our tasks.\n",
    "\n",
    "Specifically, we'll demonstrate two best practices: **Jobs** and **Experiments**.\n",
    "\n",
    "These best practices play a key role in the **Prepare Data** and **Model Development** phases of the Machine Learning Lifecycle. For more information, please refer to the [Machine Learning Best Practices in Healthcare and Life Sciences Whitepaper](https://d1.awsstatic.com/whitepapers/ML-best-practices-health-science.pdf?did=wp_card&trk=wp_card).\n",
    "\n",
    "![Machine Learning Life Cycle - Part 1](img/MLLC1.png \"ML Life Cycle - Part 1\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. SageMaker Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html) processing, training, and hyperparameter optimization (HPO) jobs allow data scientists to submit compute-heavy processes to external services. This keeps costs optimized and ensures that these tasks run in reproducible environments. It also improves data scientist productivity by allowing these jobs to run in \"the background\" and provides resiliancy if something happens to your notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/jobs.png \"Jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. SageMaker Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/experiments.png \"Experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SageMaker Experiments](https://aws.amazon.com/blogs/aws/amazon-sagemaker-experiments-organize-track-and-compare-your-machine-learning-trainings) make it as easy as possible to track data preparation and analysis steps. Organizing your ML project into experiments helps you manage large numbers of trials and alternative algorithms. Experiments also ensure that any artifacts your generate for production use can be traced back to their source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Preparation\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The Python libraries that we'll use throughout the analysis\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --disable-pip-version-check -U -q 'boto3==1.34.139' 'sagemaker==2.224.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from sagemaker.experiments.run import Run\n",
    "from sagemaker.processing import FrameworkProcessor, ProcessingOutput\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score\n",
    "from time import strftime, sleep\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Create Some Necessary Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name\n",
    "sagemaker_session = sagemaker.session.Session(boto_session)\n",
    "sagemaker_execution_role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "s3_boto_client = boto_session.client(\"s3\")\n",
    "account_id = boto_session.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "print(f\"Assumed SageMaker role is {sagemaker_execution_role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Define Experiment Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new SageMaker experiment specific to our scientific goal, in this case to predict HER2 status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"BRCA-HER2-\" + strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Specify S3 Bucket and Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = sagemaker_session.default_bucket()\n",
    "S3_PREFIX = \"brca-her2-classifier\"\n",
    "S3_PATH = sagemaker.s3.s3_path_join(S3_BUCKET, S3_PREFIX)\n",
    "print(f\"S3 path is {S3_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Define Local Working Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "print(f\"Working directory is {WORKING_DIR}\")\n",
    "print(f\"Data directory is {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preparation  with Amazon SageMaker Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Processing allows you to run steps for data pre- or post-processing, feature engineering, data validation, or model evaluation workloads on Amazon SageMaker. Processing jobs accept data from Amazon S3 as input and store data into Amazon S3 as output.\n",
    "\n",
    "![processing](https://sagemaker.readthedocs.io/en/stable/_images/amazon_sagemaker_processing_image1.png)\n",
    "\n",
    "Here, we'll import the dataset and transform it with SageMaker Processing, which can be used to process terabytes of data in a SageMaker-managed cluster separate from the instance running your notebook server. In a typical SageMaker workflow, notebooks are only used for prototyping and can be run on relatively inexpensive and less powerful instances, while processing, training and model hosting tasks are run on separate, more powerful SageMaker-managed instances.  SageMaker Processing includes off-the-shelf support for Scikit-learn, as well as a Bring Your Own Container option, so it can be used with many different data transformation technologies and tasks.    \n",
    "\n",
    "To use SageMaker Processing, simply supply a Python data preprocessing script as shown below.  For this example, we're using a SageMaker prebuilt Scikit-learn container, which includes many common functions for processing data.  There are few limitations on what kinds of code and operations you can run, and only a minimal contract:  input and output data must be placed in specified directories.  If this is done, SageMaker Processing automatically loads the input data from S3 and uploads transformed data back to S3 when the job is complete.\n",
    "\n",
    "For this example, we'll download the raw data directly from Xenahubs as part of the processing script, so we do not need to specify an input bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Submit SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take about 5 minutes to complete. Notice that this code block references `scripts/processing/processing.py`. The processing job will run this script on a different compute instance, in this case a ml.m5.xlarge. This allows us to use a small instance for our notebook server, while still taking advantage of a more powerful instance for the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HISEQ_URL = \"https://tcga.xenahubs.net/download/TCGA.BRCA.sampleMap/HiSeqV2_PANCAN.gz\"\n",
    "BRCA_CLINICAL_MATRIX_URL = (\n",
    "    \"https://tcga.xenahubs.net/download/TCGA.BRCA.sampleMap/BRCA_clinicalMatrix\"\n",
    ")\n",
    "processing_run_name = f\"data-processing-job-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "train_test_split_ratio = 0.2\n",
    "gene_count = 20000\n",
    "\n",
    "sklearn_processor = FrameworkProcessor(\n",
    "    estimator_cls=SKLearn,\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    role=sagemaker_execution_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "with Run(\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=processing_run_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "\n",
    "    sklearn_processor.run(\n",
    "        job_name=processing_run_name,\n",
    "        code=\"scripts/processing/processing.py\",\n",
    "        dependencies=[\"scripts/processing/requirements.txt\"],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/output/train\",\n",
    "                destination=f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/train/\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"validation\",\n",
    "                source=\"/opt/ml/processing/output/val\",\n",
    "                destination=f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/val/\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/output/test\",\n",
    "                destination=f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/test/\",\n",
    "            ),\n",
    "        ],\n",
    "        arguments=[\n",
    "            \"--brca_clinical_matrix_url\",\n",
    "            BRCA_CLINICAL_MATRIX_URL,\n",
    "            \"--hiseq_url\",\n",
    "            HISEQ_URL,\n",
    "            \"--train_test_split_ratio\",\n",
    "            str(train_test_split_ratio),\n",
    "            \"--gene_count\",\n",
    "            str(gene_count),\n",
    "            \"--create_test_data\",\n",
    "        ],\n",
    "        wait=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Download Processed Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.download_data(\n",
    "    f\"{DATA_DIR}/output/train\",\n",
    "    bucket=S3_BUCKET,\n",
    "    key_prefix=f\"{S3_PREFIX}/data/train/train.csv\",\n",
    ")\n",
    "sagemaker_session.download_data(\n",
    "    f\"{DATA_DIR}/output/val\",\n",
    "    bucket=S3_BUCKET,\n",
    "    key_prefix=f\"{S3_PREFIX}/data/val/val.csv\",\n",
    ")\n",
    "sagemaker_session.download_data(\n",
    "    f\"{DATA_DIR}/output/test\",\n",
    "    bucket=S3_BUCKET,\n",
    "    key_prefix=f\"{S3_PREFIX}/data/test/test.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our training data is set up, we can train some models. To highlight the benefits of experiment tracking, we're going to train models using three different frameworks:\n",
    "- The random forest model from Scikit Learm\n",
    "- A multi-layer perceptron (MLP) neural network in Keras\n",
    "- The open-source XGBoost algorithm\n",
    "\n",
    "Since we're using SageMaker jobs to run our training, we don't need to install any additional libraries or spin up expensive compute resources on our notebook server. The jobs use their own dependencies and we're only charged for the time they run.\n",
    "\n",
    "First, let's define some variables that all three training jobs will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"text/csv\"\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/train/train.csv\", content_type=content_type\n",
    ")\n",
    "\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "    f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/val/val.csv\", content_type=content_type\n",
    ")\n",
    "\n",
    "model_output_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Train Model Using a SKLearn Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again we're passing a script (`scripts/rf_train/rf_train.py`) to run during the training job. Notice that we've also included a `requirements.txt` file in the training script directory to install additional dependencies in the training container. This is a great way to install an extra package or two without creating your own container image from scratch!\n",
    "\n",
    "Setting `wait=False` allows us to continue running the notebook while the training job runs in \"the background\" (on a different machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_job_name = f\"RF-Training-Job-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "n_estimators = 100\n",
    "min_samples_leaf = 3\n",
    "\n",
    "rf_estimator = SKLearn(\n",
    "    base_job_name=rf_job_name,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    entry_point=\"rf_train.py\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    hyperparameters={\n",
    "        \"n-estimators\": n_estimators,\n",
    "        \"min-samples-leaf\": min_samples_leaf,\n",
    "    },\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    output_path=model_output_path,\n",
    "    role=sagemaker_execution_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    source_dir=\"scripts/rf_train\",\n",
    ")\n",
    "\n",
    "with Run(\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=rf_job_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "\n",
    "    rf_estimator.fit(\n",
    "        {\"train\": s3_input_train, \"validation\": s3_input_validation},\n",
    "        job_name=rf_job_name,\n",
    "        wait=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Train Model using a Keras MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_job_name = f\"TF-Training-Job-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    enable_sagemaker_metrics=True,\n",
    "    entry_point=\"tf_train.py\",\n",
    "    framework_version=\"2.2\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"validation:accuracy\", \"Regex\": \"Validation Accuracy: ([0-9.]+)$\"},\n",
    "        {\"Name\": \"validation:precision\", \"Regex\": \"Validation Precision: ([0-9.]+)$\"},\n",
    "        {\"Name\": \"validation:f1\", \"Regex\": \"Validation F1 Score: ([0-9.]+)$\"},\n",
    "    ],\n",
    "    output_path=model_output_path,\n",
    "    py_version=\"py37\",\n",
    "    role=sagemaker_execution_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    source_dir=\"scripts/tf_train\",\n",
    ")\n",
    "\n",
    "with Run(\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=tf_job_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "\n",
    "    tf_estimator.fit(\n",
    "        {\"train\": s3_input_train, \"validation\": s3_input_validation},\n",
    "        job_name=tf_job_name,\n",
    "        wait=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Train Model Using the XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the XGBoost training script we're about to run (scripts/rf_train/rf_train.py) with the training function we used in Notebook 1. You'll notice that the `xgb.train` call is the same in both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're setting `wait=True` our Jupyter session will wait until this training job is finished before moving on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_job_name = f\"XGB-Training-Job-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "framework_version = \"1.2-1\"\n",
    "py_version = \"py3\"\n",
    "\n",
    "hyper_params_dict = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"eval_metric\": \"error\",\n",
    "    \"scale_pos_weight\": 9.0,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"subsample\": 0.9,\n",
    "    \"verbosity\": 1,\n",
    "    \"tree_method\": \"auto\",\n",
    "}\n",
    "\n",
    "xgb_estimator = XGBoost(\n",
    "    enable_sagemaker_metrics=True,\n",
    "    entry_point=\"xgb_train.py\",\n",
    "    framework_version=framework_version,\n",
    "    hyperparameters=hyper_params_dict,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=model_output_path,\n",
    "    py_version=py_version,\n",
    "    role=sagemaker_execution_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    source_dir=\"scripts/xgb_train\",\n",
    ")\n",
    "\n",
    "with Run(\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=xgb_job_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "\n",
    "    xgb_estimator.fit(\n",
    "        {\"train\": s3_input_train, \"validation\": s3_input_validation},\n",
    "        job_name=xgb_job_name,\n",
    "        logs=True,\n",
    "        wait=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Compare Model Results Using SageMaker Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Experiments saves key information about our models for easy viewing and comparison. You can view the results programmatically by using the `ExperimentAnalytics` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression = {\n",
    "    \"Filters\": [\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Contains\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    experiment_name=experiment_name.lower(),\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.validation:f1.last\",\n",
    "    sort_order=\"Descending\",\n",
    "    metric_names=[\"validation:f1\"],\n",
    "    parameter_names=[\"SageMaker.InstanceType\"],\n",
    ")\n",
    "\n",
    "trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll look at options for deploying this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained a model, we can allow other applications to use it for inference by deploying it. SageMaker offers several deployment options, based on your performance, cost, and data needs:\n",
    "\n",
    "![alt text](img/deployment_options.png \"SageMaker Model Deployment Options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Deploy Model as SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-time inference endpoints are deployed to a persistent EC2 instance. This allows them to respond quickly to requests and support a wide range of custom properties. It's a good choice for models with steady usage. Deploying this endpoint will take about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "realtime_endpoint_name = f\"her2-real-time-endpoint-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "xgb_predictor = xgb_estimator.deploy(\n",
    "    endpoint_name=realtime_endpoint_name,\n",
    "    serializer=sagemaker.serializers.CSVSerializer(),  # Helper function to serialize ndarray into buffer\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),  # Helper function to deserialize buffer into ndarray\n",
    "    wait=True,\n",
    "    instance_type=\"ml.t2.medium\",  # Instance type we want to use to host our endpoint.\n",
    "    initial_instance_count=1,  # For this example, we'll only use a single hosting instance.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test traffic to the endpoint her2-real-time-endpoint-2023-01-13-14-40-53. \n",
      "Please wait...\n",
      "[Actual | predicted] labels for record  34 are [0.0 | 0.002]\n",
      "[Actual | predicted] labels for record  29 are [0.0 | 0.010]\n",
      "[Actual | predicted] labels for record  36 are [0.0 | 0.004]\n",
      "[Actual | predicted] labels for record  60 are [0.0 | 0.058]\n",
      "[Actual | predicted] labels for record  82 are [0.0 | 0.001]\n",
      "[Actual | predicted] labels for record  13 are [0.0 | 0.101]\n",
      "[Actual | predicted] labels for record 113 are [0.0 | 0.014]\n",
      "[Actual | predicted] labels for record 106 are [0.0 | 0.003]\n",
      "[Actual | predicted] labels for record  71 are [1.0 | 0.019]\n",
      "[Actual | predicted] labels for record  97 are [0.0 | 0.004]\n",
      "[Actual | predicted] labels for record  85 are [0.0 | 0.049]\n",
      "[Actual | predicted] labels for record  35 are [0.0 | 0.005]\n",
      "[Actual | predicted] labels for record  19 are [0.0 | 0.005]\n",
      "[Actual | predicted] labels for record   0 are [0.0 | 0.003]\n",
      "[Actual | predicted] labels for record  45 are [1.0 | 0.159]\n",
      "[Actual | predicted] labels for record  61 are [0.0 | 0.037]\n",
      "[Actual | predicted] labels for record 105 are [0.0 | 0.002]\n",
      "[Actual | predicted] labels for record  28 are [0.0 | 0.006]\n",
      "[Actual | predicted] labels for record  53 are [0.0 | 0.007]\n",
      "[Actual | predicted] labels for record 110 are [0.0 | 0.028]\n",
      "[Actual | predicted] labels for record   6 are [0.0 | 0.056]\n",
      "[Actual | predicted] labels for record  63 are [0.0 | 0.007]\n",
      "[Actual | predicted] labels for record  20 are [0.0 | 0.008]\n",
      "[Actual | predicted] labels for record  80 are [0.0 | 0.016]\n",
      "[Actual | predicted] labels for record  95 are [0.0 | 0.052]\n"
     ]
    }
   ],
   "source": [
    "# Load a random sample of 10 records from the test data\n",
    "test_df = pd.read_csv(\"data/output/test/test.csv\", header=None).sample(n=25)\n",
    "\n",
    "# Submit the 10 samples to the inference endpoint and compare the actual and predicted values\n",
    "print(\n",
    "    f\"Sending test traffic to the endpoint {xgb_predictor.endpoint_name}. \\nPlease wait...\"\n",
    ")\n",
    "\n",
    "for i, row in test_df.iterrows():\n",
    "    print(\n",
    "        f\"[Actual | predicted] labels for record {i:3} are [{row[0]} | {xgb_predictor.predict([row.iloc[1:]])[0]:.3f}]\"\n",
    "    )\n",
    "    sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete endpoint\n",
    "xgb_predictor.delete_endpoint()\n",
    "\n",
    "# Delete all S3 objects\n",
    "bucket = boto_session.resource(\"s3\").Bucket(S3_BUCKET)\n",
    "bucket.objects.filter().delete()\n",
    "\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"data\", ignore_errors=True)\n",
    "shutil.rmtree(\"models\", ignore_errors=True)\n",
    "shutil.rmtree(\"generated\", ignore_errors=True)\n",
    "shutil.rmtree(\"training_reports\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "fcd20795596b30c7734a8efd08df92d501ca130112f67abeee93ccff645bf25b"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
