{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee72f818-0ded-401d-b350-3dcb00346260",
   "metadata": {},
   "source": [
    "# Use Amazon SageMaker for Parameter-Efficient Fine Tuning of the ESM-2 Protein Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f779d-16c6-48ae-bc6f-290855d42346",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea37b37-5093-44ec-9235-d8ba3186bb90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Note: We recommend running this notebook on a **ml.m5.large** instance with the **Data Science 3.0** image.\n",
    "\n",
    "### What is a Protein?\n",
    "\n",
    "Proteins are complex molecules that are essential for life. The shape and structure of a protein determines what it can do in the body. Knowing how a protein is folded and how it works helps scientists design drugs that target it. For example, if a protein causes disease, a drug might be made to block its function. The drug needs to fit into the protein like a key in a lock. Understanding the protein's molecular structure reveals where drugs can attach. This knowledge helps drive the discovery of innovative new drugs.\n",
    "\n",
    "![Proteins are made up of long chains of amino acids](img/protein.png)\n",
    "\n",
    "### What is a Protein Language Model?\n",
    "\n",
    "Proteins are made up of linear chains of molecules called amino acids, each with its own chemical structure and properties. If we think of each amino acid in a protein like a word in a sentence, it becomes possible to analyze them using methods originally developed for analyzing human language. Scientists have trained these so-called, \"Protein Language Models\", or pLMs, on millions of protein sequences from thousands of organisms. With enough data, these models can begin to capture the underlying evolutionary relationships between different amino acid sequences.\n",
    "\n",
    "It can take a lot of time and compute to train a pLM from scratch for a certain task. For example, a team at Tsinghua University [recently described](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v3) training a 100 Billion-parameter pLM on 768 A100 GPUs for 164 days! Fortunately, in many cases we can save time and resources by adapting an existing pLM to our needs. This technique is called \"fine-tuning\" and also allows us to borrow advanced tools from other types of language modeling\n",
    "\n",
    "### What is LoRA?\n",
    "\n",
    "One such method originally developed in 2021 for language analysis is [\"Low-Rank Adaptation of Large Language Models\"](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v3), or \"LoRA\". This method adapts large pre-trained language models to new tasks. It does this by changing only a small part of the model. This makes the method very efficient. The small changed part targets the most important information needed for the new task. This allows quick customization of the model for new uses.\n",
    "\n",
    "`peft` is an open source library from HuggingFace to easily run parameter-efficient fine tuning jobs. That includes the use of LoRA. In addition, we'll use int-8 quantization to further increase efficiency.\n",
    "LoRA + quantization enables us to use less GPU memory (VRAM) to train large language models, giving us more compute flexibility.\n",
    "\n",
    "### What is ESM-2?\n",
    "\n",
    "[ESM-2](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1) is a pLM trained using unsupervied masked language modelling on 250 Million protein sequences by researchers at [Facebook AI Research (FAIR)](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1). It is available in several sizes, ranging from 8 Million to 15 Billion parameters. The smaller models are suitable for various sequence and token classification tasks. The FAIR team also adapted the 3 Billion parameter version into the ESMFold protein structure prediction algorithm. They have since used ESMFold to predict the struture of [more than 700 Million metagenomic proteins](https://esmatlas.com/about). \n",
    "\n",
    "ESM-2 is a powerful pLM. However, it has traditionally required multiple A100 GPU chips to fine-tune. In this notebook, we demonstrate how to use QLoRA to fine-tune ESM-2 in on an inexpensive Amazon SageMaker training instance. We will use ESM-2 to predict [subcellular localization](https://academic.oup.com/nar/article/50/W1/W228/6576357). Understanding where proteins appear in cells can help us understand their role in disease and find new drug targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75cb75-18ed-4211-aa13-3edfccd59e4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 1. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f744fde-5624-46e2-b5a5-6d6dc1c58b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q -U --disable-pip-version-check -r notebook-requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2497ac7-2c19-4668-9bf5-40c1636ce44b",
   "metadata": {},
   "source": [
    "Load the sagemaker package and create some service clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a0ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datasets import Dataset, DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sagemaker\n",
    "from sagemaker.experiments.run import Run\n",
    "from sagemaker.huggingface import HuggingFace, HuggingFaceModel\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import sagemaker_datawrangler\n",
    "from time import strftime\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "sagemaker_session = sagemaker.session.Session(boto_session)\n",
    "S3_BUCKET = sagemaker_session.default_bucket()\n",
    "s3 = boto_session.client(\"s3\")\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_execution_role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "REGION_NAME = sagemaker_session.boto_region_name\n",
    "print(f\"Assumed SageMaker role is {sagemaker_execution_role}\")\n",
    "\n",
    "S3_PREFIX = \"esm-loc-ft\"\n",
    "S3_PATH = sagemaker.s3.s3_path_join(\"s3://\", S3_BUCKET, S3_PREFIX)\n",
    "print(f\"S3 path is {S3_PATH}\")\n",
    "\n",
    "EXPERIMENT_NAME = \"esm-loc-ft-\" + strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(f\"Experiment name is {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09081bc9-1dd0-490c-a0b0-07207cbbaebc",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 2. Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b083b46-c53a-4e3b-bd3a-11cb149b5ae2",
   "metadata": {},
   "source": [
    "We'll use a version of the [DeepLoc-2 data set](https://services.healthtech.dtu.dk/services/DeepLoc-2.0/) to fine tune our localization model. It consists of several thousand protein sequences, each with one or more experimentally-observed location labels. This data was extracted by the DeepLoc team at Technical University of Denmark from the public [UniProt sequence database](https://www.uniprot.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12bb73-8673-4bf2-8f21-082751971b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://services.healthtech.dtu.dk/services/DeepLoc-2.0/data/Swissprot_Train_Validation_dataset.csv\"\n",
    ").drop([\"Unnamed: 0\", \"Partition\"], axis=1)\n",
    "df[\"Membrane\"] = df[\"Membrane\"].astype(\"int32\")\n",
    "\n",
    "# filter for sequences between 100 and 512 amino acides\n",
    "df = df[df[\"Sequence\"].apply(lambda x: len(x)).between(100, 512)]\n",
    "\n",
    "# Remove unnecessary features\n",
    "df = df[['Sequence', 'Kingdom','Membrane']]\n",
    "\n",
    "# Resample rows to randomize and create equal distribution of Membrane values\n",
    "weights = 1./df.groupby('Membrane')['Membrane'].transform('count')\n",
    "df = df.sample(n=3000, weights=weights).reset_index(drop=True)\n",
    "\n",
    "#Visualize data using the SageMaker Data Wrangler widget\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f4051-8f4d-45bb-939e-5f24cf252022",
   "metadata": {},
   "source": [
    "Next, we tokenize the sequences and trim them to a max length of 512 amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b941e433-ef3e-4d3b-9b73-19e2545eff6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98afa9c933b0482caca922766336ba21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aee0be3b27a44d5979e5f3a244509a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2700\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.1, shuffle=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t36_3B_UR50D\")\n",
    "\n",
    "def preprocess_data(examples, max_length=512):\n",
    "    text = examples[\"Sequence\"]\n",
    "    encoding = tokenizer(\n",
    "        text, \n",
    "        # padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_length\n",
    "    )\n",
    "    encoding[\"labels\"] = examples['Membrane']\n",
    "    return encoding\n",
    "\n",
    "\n",
    "encoded_dataset = dataset.map(\n",
    "    preprocess_data,\n",
    "    batched=True,\n",
    "    num_proc=os.cpu_count(),\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "encoded_dataset.set_format(\"torch\")\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33176a99-52de-45f8-ab38-c4a86812a7bc",
   "metadata": {},
   "source": [
    "Look at an example record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "472268de-85c1-4ea9-894b-8d0e6087257a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing example record 2447\n",
      "Raw sequence:\n",
      "<cls> M Q R R R R A P P A S Q P A Q D S G H S E E V E V Q F S A G R L G S A A P A G P P V R G T A E D E E R L E R E H F W K V I N A F R Y Y G T S M H E R V N R T E R Q F R S L P D N Q Q K L L P Q F P L H L D K I R K C V D H N Q E I L L T I V N D C I H M F E N K E Y G E D A N G K I M P A S T F D M D K L K S T L K Q F V R D W S G T G K A E R D A C Y K P I I K E I I K N F P K E R W D P S K V N I L V P G A G L G R L A W E I A M L G Y A C Q G N E W S F F M L F S S N F V L N R C S E V D K Y K L Y P W I H Q F S N N R R S A D Q I R P I F F P D V D P H S L P P G S N F S M T A G D F Q E I Y S E C N T W D C I A T C F F I D T A H N V I D Y I D T I W R I L K P G G I W I N L G P L L Y H F E N L A N E L S I E L S Y E D I K N V V L Q Y G F Q L E V E K E S V L S T Y T V N D L S M M K Y Y Y E C V L F V V R K P Q <eos>\n",
      "\n",
      "Tokenized sequence:\n",
      "[0, 20, 16, 10, 10, 10, 10, 5, 14, 14, 5, 8, 16, 14, 5, 16, 13, 8, 6, 21, 8, 9, 9, 7, 9, 7, 16, 18, 8, 5, 6, 10, 4, 6, 8, 5, 5, 14, 5, 6, 14, 14, 7, 10, 6, 11, 5, 9, 13, 9, 9, 10, 4, 9, 10, 9, 21, 18, 22, 15, 7, 12, 17, 5, 18, 10, 19, 19, 6, 11, 8, 20, 21, 9, 10, 7, 17, 10, 11, 9, 10, 16, 18, 10, 8, 4, 14, 13, 17, 16, 16, 15, 4, 4, 14, 16, 18, 14, 4, 21, 4, 13, 15, 12, 10, 15, 23, 7, 13, 21, 17, 16, 9, 12, 4, 4, 11, 12, 7, 17, 13, 23, 12, 21, 20, 18, 9, 17, 15, 9, 19, 6, 9, 13, 5, 17, 6, 15, 12, 20, 14, 5, 8, 11, 18, 13, 20, 13, 15, 4, 15, 8, 11, 4, 15, 16, 18, 7, 10, 13, 22, 8, 6, 11, 6, 15, 5, 9, 10, 13, 5, 23, 19, 15, 14, 12, 12, 15, 9, 12, 12, 15, 17, 18, 14, 15, 9, 10, 22, 13, 14, 8, 15, 7, 17, 12, 4, 7, 14, 6, 5, 6, 4, 6, 10, 4, 5, 22, 9, 12, 5, 20, 4, 6, 19, 5, 23, 16, 6, 17, 9, 22, 8, 18, 18, 20, 4, 18, 8, 8, 17, 18, 7, 4, 17, 10, 23, 8, 9, 7, 13, 15, 19, 15, 4, 19, 14, 22, 12, 21, 16, 18, 8, 17, 17, 10, 10, 8, 5, 13, 16, 12, 10, 14, 12, 18, 18, 14, 13, 7, 13, 14, 21, 8, 4, 14, 14, 6, 8, 17, 18, 8, 20, 11, 5, 6, 13, 18, 16, 9, 12, 19, 8, 9, 23, 17, 11, 22, 13, 23, 12, 5, 11, 23, 18, 18, 12, 13, 11, 5, 21, 17, 7, 12, 13, 19, 12, 13, 11, 12, 22, 10, 12, 4, 15, 14, 6, 6, 12, 22, 12, 17, 4, 6, 14, 4, 4, 19, 21, 18, 9, 17, 4, 5, 17, 9, 4, 8, 12, 9, 4, 8, 19, 9, 13, 12, 15, 17, 7, 7, 4, 16, 19, 6, 18, 16, 4, 9, 7, 9, 15, 9, 8, 7, 4, 8, 11, 19, 11, 7, 17, 13, 4, 8, 20, 20, 15, 19, 19, 19, 9, 23, 7, 4, 18, 7, 7, 10, 15, 14, 16, 2]\n",
      "\n",
      "Label:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "random_idx = random.randint(3, len(encoded_dataset[\"train\"]))\n",
    "example = encoded_dataset[\"train\"][random_idx]\n",
    "\n",
    "print(f\"Viewing example record {random_idx}\")\n",
    "print(f\"Raw sequence:\\n{tokenizer.decode(example['input_ids'])}\\n\")\n",
    "print(f\"Tokenized sequence:\\n{example['input_ids'].tolist()}\\n\")\n",
    "print(f\"Label:\\n{example['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519980a9-32ef-4b44-a2d5-ca54bcf89790",
   "metadata": {},
   "source": [
    "Finally, we upload the processed training, test, and validation data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dd371e0-2481-47c9-9c3b-4755fd51e4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb026d682e24446966648b8c1c868fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67ae3e5508041328d0d33fe139d41d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_s3_uri = S3_PATH + \"/data/train\"\n",
    "test_s3_uri = S3_PATH + \"/data/test\"\n",
    "\n",
    "encoded_dataset[\"train\"].save_to_disk(train_s3_uri)\n",
    "encoded_dataset[\"test\"].save_to_disk(test_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc9cf1-76d0-4c01-9dc3-b4a0f0802a03",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 3. Train model in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c1f99-4f6b-492c-9be0-973f941df9f4",
   "metadata": {},
   "source": [
    "Next, we'll process the 3 Billion-parameter model with LoRA and train on a ml.g5.2xlarge instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c5ff7-f4c2-46c5-9639-788584059586",
   "metadata": {},
   "source": [
    "Define the metrics for SageMaker to extract from the job logs and send to SageMaker Experiments. You can customize these to log more or fewer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "429cc003-7218-4933-962c-a2bb68cedf1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"'epoch': ([0-9.]*)\"},\n",
    "    {\"Name\": \"learning_rate\", \"Regex\": \"'learning_rate': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"train_loss\", \"Regex\": \"'loss': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"train_runtime\", \"Regex\": \"'train_runtime': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"train_samples_per_second\", \"Regex\": \"'train_samples_per_second': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"train_steps_per_second\", \"Regex\": \"'train_steps_per_second': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_loss\", \"Regex\": \"'eval_loss': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_runtime\", \"Regex\": \"'eval_runtime': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_samples_per_second\", \"Regex\": \"'eval_samples_per_second': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_steps_per_second\", \"Regex\": \"'eval_steps_per_second': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_accuracy\", \"Regex\": \"'eval_accuracy': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_f1\", \"Regex\": \"'eval_f1': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_roc_auc\", \"Regex\": \"'eval_roc_auc': ([0-9.e-]*)\"},\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6dd96073-8068-460a-9176-d87f232d094a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Additional training parameters\n",
    "hyperparameters = {\n",
    "    \"epochs\": 1,\n",
    "    \"model_id\": \"facebook/esm2_t6_8M_UR50D\",\n",
    "    # \"model_id\": \"facebook/esm2_t12_35M_UR50D\",\n",
    "    # \"model_id\": \"facebook/esm2_t30_150M_UR50D\",\n",
    "    # \"model_id\": \"facebook/esm2_t33_650M_UR50D\",\n",
    "    # \"model_id\": \"facebook/esm2_t36_3B_UR50D\",\n",
    "    \"lora\": True,\n",
    "    \"use_gradient_checkpointing\": True,\n",
    "}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "hf_estimator = HuggingFace(\n",
    "    base_job_name=\"esm-localization-fine-tuning\",\n",
    "    entry_point=\"lora-train.py\",\n",
    "    source_dir=\"scripts/training/peft\",\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.28\",\n",
    "    pytorch_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    output_path=f\"{S3_PATH}/output\",\n",
    "    role=sagemaker_execution_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    checkpoint_local_path=\"/opt/ml/checkpoints\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    tags=[{\"Key\": \"project\", \"Value\": \"esm-fine-tuning\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f89d80-3b2f-4360-8a47-ccf5f0120891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: esm-localization-fine-tuning-2023-10-31-03-43-52-505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-10-31 03:43:52 Starting - Starting the training job...\n",
      "2023-10-31 03:44:06 Downloading - Downloading input data\n",
      "2023-10-31 03:44:06 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:07,420 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:07,433 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:07,442 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:07,444 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:08,786 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.24.1 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.24.1-py3-none-any.whl (261 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 261.4/261.4 kB 10.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.41.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 18.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.14.6 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.6-py3-none-any.whl (493 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.7/493.7 kB 55.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 20.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-ml-py3==7.352.0 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting peft==0.5.0 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 24.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==1.3.2 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/10.8 MB 86.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.34.1 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 101.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting torchinfo==1.8.0 (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.24.1->-r requirements.txt (line 1)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->-r requirements.txt (line 3)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->-r requirements.txt (line 3)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6->-r requirements.txt (line 3)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1->-r requirements.txt (line 4)) (0.18.0)\u001b[0m\n",
      "\u001b[34mCollecting safetensors (from peft==0.5.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 86.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 7)) (1.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 7)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 7)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.1->-r requirements.txt (line 8)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub (from accelerate==0.24.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 48.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.1->-r requirements.txt (line 8)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.15,>=0.14 (from transformers==4.34.1->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 99.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 3)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 3)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 3)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.24.1->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 3)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 3)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 3)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub (from accelerate==0.24.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 45.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.24.1->-r requirements.txt (line 1)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.24.1->-r requirements.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.24.1->-r requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.24.1->-r requirements.txt (line 1)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.24.1->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nvidia-ml-py3\u001b[0m\n",
      "\u001b[34mBuilding wheel for nvidia-ml-py3 (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19171 sha256=0690426249508024fb856634f2676b66be30c422994487dd36cf40a6c8aa486e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\u001b[0m\n",
      "\u001b[34mSuccessfully built nvidia-ml-py3\u001b[0m\n",
      "\u001b[34mInstalling collected packages: nvidia-ml-py3, bitsandbytes, torchinfo, safetensors, scikit-learn, huggingface-hub, tokenizers, accelerate, transformers, datasets, peft, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scikit-learn\u001b[0m\n",
      "\u001b[34mFound existing installation: scikit-learn 1.2.2\u001b[0m\n",
      "\u001b[34mUninstalling scikit-learn-1.2.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scikit-learn-1.2.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.14.1\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.14.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.14.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: evaluate\u001b[0m\n",
      "\u001b[34mFound existing installation: evaluate 0.4.0\u001b[0m\n",
      "\u001b[34mUninstalling evaluate-0.4.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled evaluate-0.4.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.24.1 bitsandbytes-0.41.1 datasets-2.14.6 evaluate-0.4.1 huggingface-hub-0.17.3 nvidia-ml-py3-7.352.0 peft-0.5.0 safetensors-0.4.0 scikit-learn-1.3.2 tokenizers-0.14.1 torchinfo-1.8.0 transformers-4.34.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:25,020 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:25,020 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:25,052 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:25,076 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:25,099 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:25,109 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"lora\": true,\n",
      "        \"model_id\": \"facebook/esm2_t6_8M_UR50D\",\n",
      "        \"use_gradient_checkpointing\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"esm-localization-fine-tuning-2023-10-31-03-43-52-505\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-111918798052/esm-localization-fine-tuning-2023-10-31-03-43-52-505/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"lora-train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"lora-train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"lora\":true,\"model_id\":\"facebook/esm2_t6_8M_UR50D\",\"use_gradient_checkpointing\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=lora-train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=lora-train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-111918798052/esm-localization-fine-tuning-2023-10-31-03-43-52-505/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"lora\":true,\"model_id\":\"facebook/esm2_t6_8M_UR50D\",\"use_gradient_checkpointing\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"esm-localization-fine-tuning-2023-10-31-03-43-52-505\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-111918798052/esm-localization-fine-tuning-2023-10-31-03-43-52-505/source/sourcedir.tar.gz\",\"module_name\":\"lora-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"lora-train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--lora\",\"True\",\"--model_id\",\"facebook/esm2_t6_8M_UR50D\",\"--use_gradient_checkpointing\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LORA=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=facebook/esm2_t6_8M_UR50D\u001b[0m\n",
      "\u001b[34mSM_HP_USE_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 lora-train.py --epochs 1 --lora True --model_id facebook/esm2_t6_8M_UR50D --use_gradient_checkpointing True\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:25,137 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 7.38MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 775/775 [00:00<00:00, 4.97MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/31.4M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 31.4M/31.4M [00:00<00:00, 312MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m=====================================================================================\u001b[0m\n",
      "\u001b[34mLayer (type:depth-idx)                                       Param #\u001b[0m\n",
      "\u001b[34m=====================================================================================\u001b[0m\n",
      "\u001b[34mEsmForSequenceClassification                                 --\u001b[0m\n",
      "\u001b[34m├─EsmModel: 1-1                                              --\u001b[0m\n",
      "\u001b[34m│    └─EsmEmbeddings: 2-1                                    --\u001b[0m\n",
      "\u001b[34m│    │    └─Embedding: 3-1                                   10,560\u001b[0m\n",
      "\u001b[34m│    │    └─Dropout: 3-2                                     --\u001b[0m\n",
      "\u001b[34m│    │    └─Embedding: 3-3                                   328,320\u001b[0m\n",
      "\u001b[34m│    └─EsmEncoder: 2-2                                       --\u001b[0m\n",
      "\u001b[34m│    │    └─ModuleList: 3-4                                  7,397,760\u001b[0m\n",
      "\u001b[34m│    │    └─LayerNorm: 3-5                                   640\u001b[0m\n",
      "\u001b[34m│    └─EsmContactPredictionHead: 2-3                         --\u001b[0m\n",
      "\u001b[34m│    │    └─Linear: 3-6                                      121\u001b[0m\n",
      "\u001b[34m│    │    └─Sigmoid: 3-7                                     --\u001b[0m\n",
      "\u001b[34m├─EsmClassificationHead: 1-2                                 --\u001b[0m\n",
      "\u001b[34m│    └─Linear: 2-4                                           102,720\u001b[0m\n",
      "\u001b[34m│    └─Dropout: 2-5                                          --\u001b[0m\n",
      "\u001b[34m│    └─Linear: 2-6                                           642\u001b[0m\n",
      "\u001b[34m=====================================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 7,840,763\u001b[0m\n",
      "\u001b[34mTrainable params: 7,840,763\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m=====================================================================================\u001b[0m\n",
      "\u001b[34m=====================================================================================\u001b[0m\n",
      "\u001b[34mLayer (type:depth-idx)                                       Param #\u001b[0m\n",
      "\u001b[34m=====================================================================================\u001b[0m\n",
      "\u001b[34mEsmForSequenceClassification                                 --\u001b[0m\n",
      "\u001b[34m├─EsmModel: 1-1                                              --\u001b[0m\n",
      "\u001b[34m│    └─EsmEmbeddings: 2-1                                    --\u001b[0m\n",
      "\u001b[34m│    │    └─Embedding: 3-1                                   10,560\u001b[0m\n",
      "\u001b[34m│    │    └─Dropout: 3-2                                     --\u001b[0m\n",
      "\u001b[34m│    │    └─Embedding: 3-3                                   328,320\u001b[0m\n",
      "\u001b[34m│    └─EsmEncoder: 2-2                                       --\u001b[0m\n",
      "\u001b[34m│    │    └─ModuleList: 3-4                                  7,397,760\u001b[0m\n",
      "\u001b[34m│    │    └─LayerNorm: 3-5                                   640\u001b[0m\n",
      "\u001b[34m│    └─EsmContactPredictionHead: 2-3                         --\u001b[0m\n",
      "\u001b[34m│    │    └─Linear: 3-6                                      121\u001b[0m\n",
      "\u001b[34m│    │    └─Sigmoid: 3-7                                     --\u001b[0m\n",
      "\u001b[34m├─EsmClassificationHead: 1-2                                 --\u001b[0m\n",
      "\u001b[34m│    └─Linear: 2-4                                           102,720\u001b[0m\n",
      "\u001b[34m│    └─Dropout: 2-5                                          --\u001b[0m\n",
      "\u001b[34m│    └─Linear: 2-6                                           642\u001b[0m\n",
      "\u001b[34m=====================================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 7,840,763\u001b[0m\n",
      "\u001b[34mTrainable params: 7,840,763\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m=====================================================================================\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mLayer (type:depth-idx)                                                      Param #\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mPeftModelForSequenceClassification                                          --\u001b[0m\n",
      "\u001b[34m├─LoraModel: 1-1                                                            --\u001b[0m\n",
      "\u001b[34m│    └─EsmForSequenceClassification: 2-1                                    --\u001b[0m\n",
      "\u001b[34m│    │    └─EsmModel: 3-1                                                   7,829,561\u001b[0m\n",
      "\u001b[34m│    │    └─ModulesToSaveWrapper: 3-2                                       206,724\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 8,036,285\u001b[0m\n",
      "\u001b[34mTrainable params: 298,884\u001b[0m\n",
      "\u001b[34mNon-trainable params: 7,737,401\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mLayer (type:depth-idx)                                                      Param #\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mPeftModelForSequenceClassification                                          --\u001b[0m\n",
      "\u001b[34m├─LoraModel: 1-1                                                            --\u001b[0m\n",
      "\u001b[34m│    └─EsmForSequenceClassification: 2-1                                    --\u001b[0m\n",
      "\u001b[34m│    │    └─EsmModel: 3-1                                                   7,829,561\u001b[0m\n",
      "\u001b[34m│    │    └─ModulesToSaveWrapper: 3-2                                       206,724\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 8,036,285\u001b[0m\n",
      "\u001b[34mTrainable params: 298,884\u001b[0m\n",
      "\u001b[34mNon-trainable params: 7,737,401\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mPeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): EsmForSequenceClassification(\n",
      "      (esm): EsmModel(\n",
      "        (embeddings): EsmEmbeddings(\n",
      "          (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
      "        )\n",
      "        (encoder): EsmEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x EsmLayer(\n",
      "              (attention): EsmAttention(\n",
      "                (self): EsmSelfAttention(\n",
      "                  (query): Linear(\n",
      "                    in_features=320, out_features=320, bias=True\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=320, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=320, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): Linear(\n",
      "                    in_features=320, out_features=320, bias=True\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=320, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=320, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): Linear(\n",
      "                    in_features=320, out_features=320, bias=True\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=320, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=320, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (rotary_embeddings): RotaryEmbedding()\n",
      "                )\n",
      "                (output): EsmSelfOutput(\n",
      "                  (dense): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (intermediate): EsmIntermediate(\n",
      "                (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              )\n",
      "              (output): EsmOutput(\n",
      "                (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (contact_head): EsmContactPredictionHead(\n",
      "          (regression): Linear(in_features=120, out_features=1, bias=True)\n",
      "          (activation): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): EsmClassificationHead(\n",
      "          (dense): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (out_proj): Linear(in_features=320, out_features=2, bias=True)\n",
      "        )\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): EsmClassificationHead(\n",
      "            (dense): Linear(in_features=320, out_features=320, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): Linear(in_features=320, out_features=2, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 95.0/95.0 [00:00<00:00, 691kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt: 100%|██████████| 93.0/93.0 [00:00<00:00, 801kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 592kB/s]\u001b[0m\n",
      "\u001b[34m(Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2700\u001b[0m\n",
      "\u001b[34m}),)\u001b[0m\n",
      "\u001b[34m0%|          | 0/338 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\u001b[0m\n",
      "\u001b[34m0%|          | 1/338 [00:00<01:47,  3.15it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/338 [00:00<00:23, 14.40it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/338 [00:00<00:15, 20.88it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 13/338 [00:00<00:12, 25.15it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6931, 'learning_rate': 4.7633136094674555e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/338 [00:00<00:12, 25.15it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 17/338 [00:00<00:11, 27.26it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/338 [00:00<00:10, 29.10it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/338 [00:01<00:10, 31.16it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 29/338 [00:01<00:09, 32.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6861, 'learning_rate': 4.5266272189349114e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 32/338 [00:01<00:09, 32.46it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 33/338 [00:01<00:09, 32.28it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/338 [00:01<00:09, 32.82it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/338 [00:01<00:08, 33.01it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/338 [00:01<00:08, 33.08it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.698, 'learning_rate': 4.289940828402367e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 48/338 [00:01<00:08, 33.08it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/338 [00:01<00:08, 33.34it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 53/338 [00:01<00:08, 33.24it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 57/338 [00:01<00:08, 33.75it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 61/338 [00:02<00:07, 35.30it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6867, 'learning_rate': 4.053254437869823e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 64/338 [00:02<00:07, 35.30it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 65/338 [00:02<00:07, 35.51it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 69/338 [00:02<00:07, 35.42it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 73/338 [00:02<00:07, 35.20it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 77/338 [00:02<00:07, 35.15it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6891, 'learning_rate': 3.8165680473372784e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 80/338 [00:02<00:07, 35.15it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 81/338 [00:02<00:07, 34.87it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 85/338 [00:02<00:07, 35.28it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 89/338 [00:02<00:07, 34.67it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 93/338 [00:02<00:07, 34.18it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6998, 'learning_rate': 3.5798816568047336e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 96/338 [00:03<00:07, 34.18it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 97/338 [00:03<00:07, 34.10it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 101/338 [00:03<00:06, 35.14it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 105/338 [00:03<00:06, 34.75it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 109/338 [00:03<00:06, 34.88it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6866, 'learning_rate': 3.3431952662721895e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 112/338 [00:03<00:06, 34.88it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 113/338 [00:03<00:06, 34.42it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 117/338 [00:03<00:06, 34.72it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 121/338 [00:03<00:06, 34.33it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 125/338 [00:03<00:06, 34.40it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.69, 'learning_rate': 3.106508875739645e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 128/338 [00:04<00:06, 34.40it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 129/338 [00:04<00:06, 34.55it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 133/338 [00:04<00:05, 35.15it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 137/338 [00:04<00:05, 34.74it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 141/338 [00:04<00:05, 35.28it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6927, 'learning_rate': 2.869822485207101e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 144/338 [00:04<00:05, 35.28it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 145/338 [00:04<00:05, 35.54it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 149/338 [00:04<00:05, 35.63it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 153/338 [00:04<00:05, 35.85it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 157/338 [00:04<00:05, 35.58it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6821, 'learning_rate': 2.6331360946745565e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 160/338 [00:04<00:05, 35.58it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 161/338 [00:04<00:05, 34.29it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 165/338 [00:05<00:05, 33.18it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 169/338 [00:05<00:05, 32.56it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 173/338 [00:05<00:05, 31.85it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6886, 'learning_rate': 2.396449704142012e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 176/338 [00:05<00:05, 31.85it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 177/338 [00:05<00:05, 31.88it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 181/338 [00:05<00:04, 32.24it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 185/338 [00:05<00:04, 32.62it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 189/338 [00:05<00:04, 32.26it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6859, 'learning_rate': 2.1597633136094676e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 192/338 [00:05<00:04, 32.26it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 193/338 [00:05<00:04, 32.17it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 197/338 [00:06<00:04, 33.18it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 201/338 [00:06<00:03, 34.73it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 205/338 [00:06<00:03, 35.63it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6881, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 208/338 [00:06<00:03, 35.63it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 209/338 [00:06<00:03, 34.40it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 213/338 [00:06<00:03, 34.77it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 217/338 [00:06<00:03, 34.82it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 221/338 [00:06<00:03, 35.32it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.686, 'learning_rate': 1.6863905325443787e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 224/338 [00:06<00:03, 35.32it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 225/338 [00:06<00:03, 35.29it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 229/338 [00:06<00:03, 35.70it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 233/338 [00:07<00:02, 35.38it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 237/338 [00:07<00:02, 35.81it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6883, 'learning_rate': 1.4497041420118343e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 240/338 [00:07<00:02, 35.81it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 241/338 [00:07<00:02, 35.14it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 245/338 [00:07<00:02, 34.57it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 249/338 [00:07<00:02, 34.77it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 253/338 [00:07<00:02, 34.73it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6862, 'learning_rate': 1.21301775147929e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 256/338 [00:07<00:02, 34.73it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 257/338 [00:07<00:02, 35.49it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 261/338 [00:07<00:02, 35.31it/s]\u001b[0m\n",
      "\n",
      "2023-10-31 03:44:52 Uploading - Uploading generated training model\u001b[34m78%|███████▊  | 265/338 [00:07<00:02, 35.72it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 269/338 [00:08<00:01, 35.89it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6885, 'learning_rate': 9.763313609467455e-06, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|████████  | 272/338 [00:08<00:01, 35.89it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 273/338 [00:08<00:01, 35.00it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 277/338 [00:08<00:01, 34.93it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 282/338 [00:08<00:01, 36.54it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 286/338 [00:08<00:01, 35.79it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.684, 'learning_rate': 7.396449704142013e-06, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 288/338 [00:08<00:01, 35.79it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 290/338 [00:08<00:01, 34.98it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 294/338 [00:08<00:01, 34.62it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 298/338 [00:08<00:01, 34.63it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 302/338 [00:09<00:01, 35.18it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6837, 'learning_rate': 5.029585798816568e-06, 'epoch': 0.9}\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 304/338 [00:09<00:00, 35.18it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 306/338 [00:09<00:00, 35.11it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 310/338 [00:09<00:00, 34.93it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 314/338 [00:09<00:00, 34.24it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 318/338 [00:09<00:00, 34.20it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6897, 'learning_rate': 2.6627218934911246e-06, 'epoch': 0.95}\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 320/338 [00:09<00:00, 34.20it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 322/338 [00:09<00:00, 34.08it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 326/338 [00:09<00:00, 34.22it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 330/338 [00:09<00:00, 35.57it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 334/338 [00:09<00:00, 35.80it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6852, 'learning_rate': 2.958579881656805e-07, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 336/338 [00:10<00:00, 35.80it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 338/338 [00:10<00:00, 36.22it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/19 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 4/19 [00:00<00:00, 27.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 7/19 [00:00<00:00, 23.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 10/19 [00:00<00:00, 21.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 13/19 [00:00<00:00, 21.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 16/19 [00:00<00:00, 21.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 19/19 [00:00<00:00, 21.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6867578029632568, 'eval_accuracy': 0.5833333333333334, 'eval_runtime': 0.9294, 'eval_samples_per_second': 322.773, 'eval_steps_per_second': 20.442, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 338/338 [00:10<00:00, 36.22it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 19/19 [00:00<00:00, 21.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'train_runtime': 10.9837, 'train_samples_per_second': 245.818, 'train_steps_per_second': 30.773, 'train_loss': 0.6885747965976331, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 338/338 [00:10<00:00, 36.22it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 338/338 [00:10<00:00, 30.78it/s]\u001b[0m\n",
      "\u001b[34mMax GPU memory use during training: 3108 MB\u001b[0m\n",
      "\u001b[34mSome weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:43,164 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:43,164 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-31 03:44:43,164 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-10-31 03:45:04 Completed - Resource retained for reuse\n"
     ]
    }
   ],
   "source": [
    "with Run(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "    hf_estimator.fit(\n",
    "        {\n",
    "            \"train\": TrainingInput(s3_data=train_s3_uri, input_mode=\"File\"),\n",
    "            \"test\": TrainingInput(s3_data=test_s3_uri, input_mode=\"File\"),\n",
    "        },\n",
    "        wait=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba110cf-b9fe-4e70-8443-671a3a8a88fc",
   "metadata": {},
   "source": [
    "You can view metrics and debugging information for this run in SageMaker Experiments. On the left-side navigation panel, select the Home icon, then \"Experiments\". From there, you can select your experiment name and training job name and view the Debugger insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce065c-99e3-47ca-bac0-6ba51ee0925a",
   "metadata": {},
   "source": [
    "While the training job is running, take a look at the training script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca12bb0-8bec-4e33-b2a6-0542ba13b50e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deploy Model as Real-Time Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e17487-bd97-4cc6-a116-06945d15402c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=hf_estimator.model_uri,\n",
    "    role=sagemaker_execution_role,\n",
    "    transformers_version=\"4.28.1\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    env={\"HF_TASK\": \"text-classification\"},\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\"\n",
    "    role=sagemaker_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ec68e-b12f-4dc6-a58b-947cc494ad5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_seq = \"MAAAVVLAAGLRAARRAVAATGVRGGQVRGAAGVTDGNEVAKAQQATPGGAAPTIFSRILDKSLPADILYEDQQCLVFRDVAPQAPVHFLVIPKKPIPRISQAEEEDQQLLGHLLLVAKQTAKAEGLGDGYRLVINDGKLGAQSVYHLHIHVLGGRQLQWPPG\"\n",
    "sample = {\"inputs\": test_seq}\n",
    "predictor.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4e10a-0152-4e03-931c-cb2820a8e337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e573c-4c2d-4fc8-bb22-827342f47666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2752bddc-1640-42da-a762-255acd24730d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4657f-3073-4305-a9a0-411a36826487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
